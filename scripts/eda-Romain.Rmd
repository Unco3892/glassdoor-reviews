---
title: "EDA-Romain"
output: html_document
---

```{r setup, include=FALSE,message=FALSE}
source(here::here("scripts/setup.R"))
```

# Exploratory Data Analysis ROMAIN

## Complete pros reviews set (each review is a doc)
```{r ,message=FALSE}
library(lexicon)

#importation of the corpus
reviews.complete <- read_csv(here::here("data/Bank_reviews_processed.csv"))

#corpus for the pros
reviews.corpus.pro <- corpus(x = reviews.complete,
                             text_field = c("employer_pros"))

### tokenization
library(lavaan)

#Romain: no need for word1 argument, as we are not concerned by symbols or whatever.
reviews.tokens.pro <- tokens(reviews.corpus.pro, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        what="word")

# We decide to remove words that won't bring additional information, such as the name of the bank, etc.
to.be.removed <- c("ubs","jp","chase","td","morgan","hsbc","deutsche","db","jpmc","j.p","jpmorgan")

#Use of lemmitization
reviews.tokens.pro <- tokens_tolower(reviews.tokens.pro) %>% tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>% tokens_remove(c(stopwords("english"),to.be.removed))
```



## PROS sorted by bank
```{r}
reviews.pro.dfm <- dfm(reviews.tokens.pro)
reviews.company <-reviews.complete$company
reviews.pro.dfm$company <-reviews.company
reviews.bank.pro.dfm<-dfm_group(reviews.pro.dfm,groups = "company")

#frequency per terms
reviews.bank.pro.freq <- textstat_frequency(reviews.bank.pro.dfm,groups = "company")

#tf_idf
reviews.bank.pro.dfm.tfidf<-dfm_tfidf(reviews.bank.pro.dfm)

index <- reviews.bank.pro.freq %>% top_n(15)
index <- index %>% filter(rank<15)
reviews.bank.pro.freq %>% 
  filter(feature %in% index$feature) %>% 
  ggplot(aes(feature, frequency)) + 
  geom_col() + 
  coord_flip() +
  facet_wrap(~group, ncol = 3) +
  labs(title="Whole dataset top 15 token frequency by bank")
```
The top 15 token frequency plot do not bring many insights, as the frequency of each token is almost similar through the bank. However, the token "benefit" is more frequent for TD and J.P. Morgan

### Keyness analysis for each bank compared to the other
```{r}
company <- docnames(reviews.bank.pro.dfm)
res <- numeric(length = length(company))

for (i in 1:length(company)) {
  text2.kn <- textstat_keyness(reviews.bank.pro.dfm, target = i)
  print(textplot_keyness(text2.kn))
}
```

### Compare the reviews in terms of lexical diversity
```{r}
textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "I") %>%
  ggplot(aes(x = reorder(document, I), y = I)) +
  geom_point() +
  coord_flip() +
  xlab("Text") +
  ylab("Yule's index")

textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "TTR") %>%
  ggplot(aes(x = reorder(document, TTR), y = TTR)) +
  geom_point() +
  coord_flip() +
  xlab("Text") +
  ylab("TTR's index")


textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "MATTR") %>%
  ggplot(aes(x = reorder(document, MATTR), y = MATTR)) +
  geom_point() +
  coord_flip() +
  xlab("Text") +
  ylab("MATTR's index")
```

### Job analysis
Here my goal is to see if jobs positions are significantly different from one company to another
```{r}
# We select and clean variable employee_role
reviews.complete <-
  separate(
    data = reviews.complete,
    col = employee_role,
    into = c(NA, "employee_role"),
    sep = "\\-"
  )

jobs.corpus <- corpus(x = reviews.complete,
                      text_field = c("employee_role"))

jobs.tokens <- tokens(
  jobs.corpus,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  what = "sentence"
)

jobs.dfm <- dfm(jobs.tokens) # create dfm
jobs.dfm$company <- reviews.complete$company
jobs.dfm <- dfm_group(jobs.dfm, groups = "company") %>% 
  dfm_sort(decreasing=TRUE)

# We filter the unapropriate terms
to.be.removed <- c(" anonymous employee",
          " teller ii",
          " teller i" ,
          " vice president",
          " assistant vice president",
          " director",
          " associate director"
          )
jobs.dfm <- jobs.dfm %>% dfm_remove(to.be.removed)
jobs.dfm <- jobs.dfm[,1:50] #sort dfm by only keeping most used term for better display

# We display position frequency by bank

jobs.freq <- textstat_frequency(jobs.dfm,groups = "company")
index.jobs <- jobs.freq 

jobs.freq %>%
  arrange(desc(frequency)) %>%
  filter(rank<=5) %>%
  ggplot(aes(x=feature, y=frequency)) + 
  geom_col() + 
  coord_flip() +
  facet_wrap(~group, ncol = 3) +
  labs(title="Top 5 most frequent position for each bank") + xlab("") +ylab("")

# Representation of the job position on the biplot
tmod <- textmodel_lsa(jobs.dfm, nd = 3)

biplot(
  y = tmod$docs[, 2:3],
  x = tmod$features[, 2:3],
  col = c("grey", "red"),
  xlab = "Dim 2",
  ylab = "Dim 3",
  cex=0.8
)

```






# Sentiment analysis for the pros reviews
Trying to understand if positive reviews employee sentiments vary from bank to bank.
Obviously, some reviews are really short and do not contain any sentiment. However, I suppose that some reviews are more develloped and could potentially contain sentiment/feeling that could be interesting.
```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(readr)

# We need to create one vector/bank which contain all the review
# I create a function, however, it should be possible to do it using nest function (dplyr), but laziness

# We create a function that extract each review and aggregate them in one vector
merge.review.pro <- function(.Data, companyname) {
  .Data %>%
    filter(company == companyname) %>%
    select(employer_pros) %>%
    str_c(sep = " ", collapse = TRUE)
}

merge.review.con <- function(.Data, companyname) {
  .Data %>%
    filter(company == companyname) %>%
    select(employer_cons) %>%
    str_c(sep = " ", collapse = TRUE)
}

# We iterate this function for each bank
company.name <- unique(reviews.complete$company)
reviews.bank.pro <- c("", "", "", "", "")
reviews.bank.con <- c("", "", "", "", "")

for (i in 1:length(company.name)) {
  reviews.bank.pro[i] <-
    merge.review.pro(reviews.complete, company.name[i])
}

for (i in 1:length(company.name)) {
  reviews.bank.con[i] <-
    merge.review.con(reviews.complete, company.name[i])
}

# We merge the prosand cons together and prepare the corpus
reviews.bank <- paste(reviews.bank.pro, reviews.bank.con, sep = "")
review.cp <- corpus(reviews.bank)

# We tokenize and assign the sentiment dictionnary to every pertinent token
review.tk <-
  review.cp %>% tokens(
    remove_numbers = TRUE,
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_separators = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_replace(pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(stopwords("english"))

review.sent <-
  tokens_lookup(review.tk, dictionary = data_dictionary_LSD2015) %>%
  dfm() %>%
  tidy()

# We replace the name of docs by the company name
company.df <- t(t(company.name))
review.sent$document <- rbind(company.df, company.df)

# We scale the data to have comparable results
review.sent.scale <- review.sent %>%
  group_by(document) %>%
  mutate (factor = 100 / sum(count)) %>%
  mutate(percent = (count * factor) / 100)

# We plot the results
review.sent %>% ggplot(aes(x = document, y = count, fill = term)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  ylab("") +
  labs(title = "Sentiment Analysis (absolute value)", fill = "")

# We plot the results
review.sent.scale %>% ggplot(aes(x = document, y = percent, fill = term)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  ylab("") +
  labs(title = "Sentiment Analysis (relative value)", fill = "")
```


