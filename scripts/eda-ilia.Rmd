---
title: "EDA-Ilia"
output: html_document
---

```{r setup, include=FALSE}
# Loading packages and knitr options
source(here::here("scripts/setup.R"))

# Importing the data
bank_reviews <- read_csv(here::here("data/Bank_reviews_processed.csv"))
```

## Definitions

`Corpus` = All reviews on GlassDoor for five major banks
`Text` = Reviews for each bank --> Used this also as the `Document`
`Tokens` = Words in each review

### Data structure

```{r}
bank_reviews %>% glimpse
```

The dataset consists of `r length(unique(bank_reviews$company))` banks which are `r unique(bank_reviews$company) %>% paste(.,collapse = ", ")`.

We can summarize the information for each bank.

NOTE TO ilia: Replace the function below with an across as summarize_if is deprecated.

Checking for NAs and duplicates.

```{r}
freq.na(bank_reviews)

sum(duplicated(bank_reviews$review_id)) #The output shows that 429 reviews are duplicated, let's check them

# All the row's who values are duplicated
bank_reviews %>%
  filter(duplicated(review_id))

# These are all the duplicated values
bank_reviews %>% group_by(review_id) %>% filter(n()>1) %>% ungroup()

# We look at a specific example
bank_reviews %>% filter(review_id == "empReview_38099527")

# We will remove these rows
bank_reviews %<>%
  filter(!duplicated(review_id))

# We confirm that the duplicates are no longer there
bank_reviews %>% filter(review_id == "empReview_38099527")
```


```{r}
# General overview
bank_reviews %>%
  select(company, employer_rating:senior_management) %>%
  group_by(company) %>%
  add_count() %>%
  summarize_if(is.numeric,
               c(
                 "mean" = function(x)
                   mean(x, na.rm = TRUE),
                 "median" = function(x)
                   median(x, na.rm = TRUE)
               )) %>%
  relocate(company, n_median) %>%
  rename("number of reviews" = n_median) %>%
  kable_maker()

# An alternative shorter approach
# bank_reviews %>% 
#   dplyr::count(company, name = "number of reviews") %>% 
#   kable_maker()

# calculating the mean number of words
mean_words_cal <- function(a_word_column) {
  mean(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]+"
  ), length))
}

median_words_cal <- function(a_word_column) {
  median(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]+"
  ), length))
}

last_tenth <- function(a_word_column) {
  quantile(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]+"
  ), length), 0.9, na.rm = TRUE)
}

# applying the function to the two word columns
bank_reviews %>% group_by(company) %>% 
  summarize(mean_pro_words = mean_words_cal(employer_pros),
            mean_con_words = mean_words_cal(employer_cons),
            median_pro_words = median_words_cal(employer_pros),
            median_con_words = median_words_cal(employer_cons),
            last_tenth_pros = last_tenth(employer_pros),
            last_tenth_cons = last_tenth(employer_cons)
            ) %>% 
  View()

```

## EDA
A general EDA for the reviews Note that here we treat each company as a document.

```{r}
# creating function to take out the pros and cons of the reviews
EDA_handler <-
  function (a_tibble,
            text_column,
            n_top = 20,
            TF_grouped = FALSE,
            TF_IDF = FALSE,
            word_cloud = FALSE,
            aggregate_cloud = FALSE) {
    # creating the new column name
    new_col <- paste0(quo_name(enquo(text_column)), "_word")
    
    # tokenization of the reviews
    review_token <- tidytext::unnest_tokens(
      a_tibble,
      output = "word",
      input = {{text_column}},
      to_lower = TRUE,
      strip_punct = TRUE,
      strip_numeric = TRUE
    ) %>%
      # removing the stop words
      anti_join(., stop_words, by = "word") %>%
      rename(., !!(new_col) := "word")
    
    # looking at the most frequent words
    freq <- review_token %>%
      group_by(company) %>%   # grouping is also possible also with review_id
      count("{{text_column}}_word" := get(new_col), sort = TRUE) %>%
      ungroup()
    
    # computing the TF-IDF if the option is set as TRUE
    if (TF_IDF == TRUE) {
      reviews_TF_IDF <- tidytext::bind_tf_idf(
        tbl = freq,
        term = !!(new_col),
        document = company,
        n = n
      )
    }
    
    # indexing for the top n words and also if it was a TFI-IDF then use that
    # for the index instead of the the typical frequency table
    index <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      `if`(TF_grouped, {.} %>% group_by(company), .) %>%
      top_n(n_top)
    
    # creating a function to be used for the ggplot aesthetics
    fix_lables <- function (a_column) {
      stri_replace_all_fixed(a_column, c("_", "s"), c(" ", ""), vectorize_all = FALSE)
    }
    
    # designing a ggplot for the most frequent words
    freq_plt <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      filter(get(new_col) %in% (index %>% pull(get(new_col)))) %>%
      ggplot(aes(x = get(new_col), y = `if`(TF_IDF, tf_idf, n))) + 
      geom_col() + 
      coord_flip() + 
      facet_grid( ~ company) + 
      theme_light() +
      # another alternative for the wrap --> facet_wrap(~company, ncol = 2)
      labs(
        title = paste0("Most frequent ",
                       fix_lables(new_col),
                       "s per bank"),
        y = `if`(TF_IDF, "TF_IDF", NULL),
        x = fix_lables(new_col)
      )
    
    # creating wordcloud if the option is set equal to TRUE
    if (word_cloud == TRUE) {
      set.seed(1234)
      dev.new(width = 1000, height = 1000, unit = "px")
      #many warnings are generated on this which is normal
      if (aggregate_cloud == TRUE) {
        freq_agg <- aggregate(as.formula(paste("n", "~", new_col, sep= " "))
                              , FUN = sum, data=freq)
      }
      clouds <-
        wordcloud::wordcloud(
          words = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(!!new_col)),
          freq = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(n)),
          min.freq = 2,
          max.words = 10,
          random.order = FALSE,
          random.color = FALSE,
          colors = brewer.pal(8, "Dark2"))
    }
    
    # generating the output
    output <-
      list(
        "Removing_stopwords" = review_token,
        "Most_frequent_words" = freq,
        if(exists("clouds")){"Word_cloud" = clouds},
        "Plotting_most_frequent_words" = freq_plt
      )
    
    return (output)
  }


# TF approach
pros_tf <- EDA_handler(bank_reviews, employer_pros)
cons_tf <- EDA_handler(bank_reviews, employer_cons)


# TF_IDF approach
pros_tf_idf <- EDA_handler(bank_reviews, employer_pros, n_top = 5,TF_grouped= TRUE, TF_IDF = TRUE)
cons_tf_idf <- EDA_handler(bank_reviews, employer_cons, n_top = 1,TF_grouped= TRUE, TF_IDF = TRUE)

# The grouped and specific to each document --> Only one word is shown which is quite strange and requires more investigation
pros_grouped <- EDA_handler(bank_reviews, employer_pros, n_top = 1, TF_grouped= TRUE)

# Finally, for the wordcloud, firstly it takes a while to render and then, there are two problems with it, a) it does not generate the word management for cons and b, if outputs as `null` all the time, for that to be resolved, ggwordcloud library could be used as explained by the code below.
pros_cloudy <- EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = TRUE)

# aggregated wordcloud can also be produced.
pros_cloudy_aggregated <- EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = T, aggregate_cloud = TRUE)

# The code that can be used as a wordcloud replacement is the following.
  #   if (word_cloud == TRUE) {
  #     top_words <- freq %>% top_n(100, wt= n)
  #     cloudy_words <-
  #       ggplot(top_words, aes(label = get(new_col), size =n)) +
  # geom_text_wordcloud(area_corr = TRUE, rm_outside = TRUE)+
  #     # eccentricity = 0.35
  # scale_size_area(max_size = 50) +
  #   theme_minimal()
  #     }
  #   cloudy_words
  # }

# what more to be done?
# try with lemmatization approach
# lexical diversity
# compute keynesses
```

The ggplot represents the most frequent words per company as the grouping was not done on the review but the company itself.

Regarding the pros, most often the words `benefit` and `people` come up, it is also interesting to see that the words `culture`, and `life` are also there perhaps indicating that most people care about these values when trying to describe work positively.

We can see that the most cons were associated with the word `management`. This is followed by `employees`, `hour` and `time`. Furthermore we have the same `people` word in the cons meaning that we would have to put our analysis into contenxt and use the valence shifters to see if they mention something good about people or bad about these people.

See wordcloud 2 with the tm package from here https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a
library(wordcloud2)

<!-- See on average how many words each review contains -->

Also `quanteda` library could be used if needed..

```{r}
pros_corpus <- bank_reviews %>% select(company, review_id, employer_pros) %>% corpus(text_field = "employer_pros")
```



Trying out some things with sentiment analysis and dictionaries.

```{r}
pro_sentiments <- pros_tf$Removing_stopwords %>% rename(word= employer_pros_word)

con_sentiments <- cons_tf$Removing_stopwords %>% rename(word= employer_cons_word)


# Dictionaries for categories
get_sentiments("nrc")

crude_sentiment <- pro_sentiments %>% inner_join(get_sentiments("nrc"))

crude_sentiment %>% group_by(company, sentiment) %>%  summarize(n = n()) %>% 
  mutate(freq = n / sum(n)) %>% ggplot(aes(x = sentiment, y = freq, fill = sentiment)) + 
  geom_bar(stat = "identity", alpha = 0.8) + 
  facet_wrap(~ company) + coord_flip()

# Dictionaries by values
get_sentiments("afinn")

library("textstem")

# Creating the lemmatization
pro_sentiments %<>% mutate(word_lemma = textstem::lemmatize_strings(word))
 
con_sentiments %<>% mutate(word_lemma = textstem::lemmatize_strings(word))

values <-  pro_sentiments %>% inner_join(get_sentiments("afinn"))

con_values <- con_sentiments %>% inner_join(get_sentiments("afinn"))

# Looking at the values generated
aggregate(value~company, data=values, FUN=mean) %>% 
  ggplot(aes(x = company, y = value)) + 
  geom_bar(stat = "identity") + coord_flip()


# Mean employer rating existing in the raw data
existing_score <- aggregate(employer_rating~company, data=bank_reviews, FUN=mean) %>% arrange(desc(employer_rating))

# Highest semantic (positive)
pos_sem <- aggregate(value~company, data=values, FUN=mean)  %>% 
  arrange(desc(value))

# Highest semantic (negative)
neg_sem <- aggregate(value~company, data=con_values, FUN=mean)  %>% 
  arrange(desc(value))

# Both semantics combined
overall_score <- pos_sem %>% left_join(neg_sem, by = "company") %>% 
  mutate(score_numerical_analysis = value.x+value.y) %>% 
  select(-c(2,3))

# Comparing the two scores
existing_score %>% left_join(overall_score) %>% arrange(desc(2))
```

More advanced methods with valence shifters

```{r}
library(sentimentr)
library(lexicon)

mytext <- get_sentences(bank_reviews$employer_pros)

sentiments <- sentiment(mytext)

# Both semantics combined
overall_score <- pos_sem %>% left_join(neg_sem, by = "company") %>% 
  mutate(score_numerical_analysis = value.x+value.y) %>% 
  select(-c(2,3))

# Comparing the two scores
existing_score %>% left_join(overall_score, sentiments) %>% arrange(desc(2))

```