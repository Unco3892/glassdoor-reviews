---
title: "EDA-Ilia"
output: html_document
---

```{r setup, include=FALSE}
# Loading packages and knitr options
source(here::here("scripts/setup.R"))

# Importing the data
bank_reviews <- read_csv(here::here("data/Bank_reviews_processed.csv"))

```

## Definitions

`Corpus` = All reviews on GlassDoor for five major banks
`Text` = Reviews for each bank --> Used this also as the `Document`
`Tokens` = Words in each review

### Data structure

```{r}
bank_reviews %>% glimpse
```

The dataset consists of `r length(unique(bank_reviews$company))` banks which are `r unique(bank_reviews$company) %>% paste(.,collapse = ", ")`.

We can summarize the information for each bank.

NOTE TO ilia: Replace the function below with an across as summarize_if is deprecated.

Checking for NAs and duplicates.

```{r}
freq.na(bank_reviews)

sum(duplicated(bank_reviews$review_id)) #The output shows that 429 reviews are duplicated, let's check them

# All the row's who values are duplicated
bank_reviews %>%
  filter(duplicated(review_id))

# These are all the duplicated values
bank_reviews %>% group_by(review_id) %>% filter(n()>1) %>% ungroup()

# We look at a specific example
bank_reviews %>% filter(review_id == "empReview_38099527")

# We will remove these rows
bank_reviews %<>%
  filter(!duplicated(review_id))

# We confirm that the duplicates are no longer there
bank_reviews %>% filter(review_id == "empReview_38099527")
```


```{r}
# General overview
bank_reviews %>%
  select(company, employer_rating:senior_management) %>%
  group_by(company) %>%
  add_count() %>%
  summarize_if(is.numeric,
               c(
                 "mean" = function(x)
                   mean(x, na.rm = TRUE),
                 "median" = function(x)
                   median(x, na.rm = TRUE)
               )) %>%
  relocate(company, n_median) %>%
  rename("number of reviews" = n_median) %>%
  kable_maker()

# An alternative shorter approach
# bank_reviews %>% 
#   dplyr::count(company, name = "number of reviews") %>% 
#   kable_maker()

# calculating the mean number of words
mean_words_cal <- function(a_word_column) {
  mean(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]"
  ), length))
}

median_words_cal <- function(a_word_column) {
  median(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]"
  ), length))
}

quantile_fun <- function(a_word_column,a_quantile) {
  quantile(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]"
  ), length), a_quantile, na.rm = TRUE)
}

# applying the function to the two word columns
bank_reviews %>% group_by(company) %>%
  summarize(across(
    c(employer_pros, employer_cons),
    c(
      "1st_Qu." = function(x)quantile_fun(x, 0.25),
      "Median" = median_words_cal,
      "Mean" = mean_words_cal,
      "3rd_Qu." = function(x) quantile_fun(x, 0.75)
    )
  )) %>%
  gather(metric, val, 2:ncol(.)) %>%
  spread(company, val) %>% 
  kable_maker()

```

## EDA
### General explorations
A general EDA for the reviews Note that here we treat each company as a document.

```{r}
# creating function to take out the pros and cons of the reviews
EDA_handler <-
  function (a_tibble,
            text_column,
            n_top = 20,
            TF_grouped = FALSE,
            TF_IDF = FALSE,
            word_cloud = FALSE,
            aggregate_cloud = FALSE,
            lemma=FALSE) {
    # creating the new column name
    new_col <- paste0(quo_name(enquo(text_column)), "_word")
    # tokenization of the reviews
    review_token <- tidytext::unnest_tokens(
      a_tibble,
      output = "word",
      input = {{text_column}},
      to_lower = TRUE,
      strip_punct = TRUE,
      strip_numeric = TRUE) %>%
      # if the lemmatization option is activated
      `if`(lemma,{.} %>% mutate(word = lemmatize_strings(word)),.) %>%
      # removing the stop words
      anti_join(., stop_words, by = "word") %>%
      rename(., !!(new_col) := "word")
    # looking at the most frequent words
    freq <- review_token %>%
      group_by(company) %>%   # grouping is also possible also with review_id
      count("{{text_column}}_word" := get(new_col), sort = TRUE) %>%
      ungroup()
    # computing the TF-IDF if the option is set as TRUE
    if (TF_IDF == TRUE) {
      reviews_TF_IDF <- tidytext::bind_tf_idf(
        tbl = freq,
        term = !!(new_col),
        document = company,
        n = n
      )
    }
    # indexing for the top n words and also if it was a TFI-IDF then use that
    # for the index instead of the the typical frequency table
    index <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      `if`(TF_grouped, {.} %>% group_by(company), .) %>%
      top_n(n_top)
    # creating a function to be used for the ggplot aesthetics
    fix_lables <- function (a_column) {
      stri_replace_all_fixed(a_column, c("_", "s"), c(" ", ""), vectorize_all = FALSE)
    }
    # designing a ggplot for the most frequent words
    freq_plt <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      filter(get(new_col) %in% (index %>% pull(get(new_col)))) %>%
      ggplot(aes(x = get(new_col), y = `if`(TF_IDF, tf_idf, n), fill= get(new_col))) + 
      geom_col() + 
      coord_flip() + 
      facet_grid( ~ company) + 
      theme_light() +
      # another alternative for the wrap --> facet_wrap(~company, ncol = 2)
      labs(
        title = paste0("Most frequent ",
                       fix_lables(new_col),
                       "s per bank"),
        y = `if`(TF_IDF, "TF_IDF", NULL),
        x = fix_lables(new_col)
      )
    
    # creating wordcloud if the option is set equal to TRUE
    if (word_cloud == TRUE) {
      set.seed(1234)
      # dev.new(width = 1000, height = 1000, unit = "px")
      #many warnings are generated on this which is normal
      if (aggregate_cloud == TRUE) {
        freq_agg <- aggregate(as.formula(paste("n", "~", new_col, sep= " "))
                              , FUN = sum, data=freq)
      }
      clouds <-
        wordcloud::wordcloud(
          words = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(!!new_col)),
          freq = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(n)),
          min.freq = 2,
          max.words = 10,
          random.order = FALSE,
          random.color = FALSE,
          colors = brewer.pal(8, "Dark2"))
    }
    
    # generating the output
    output <-
      list(
        review_token,
        # "Remove_stopwords" = review_token,
        "Most_frequent_words" = freq,
        # if(exists("clouds")){"Word_cloud" = clouds},
        "Plotting_most_frequent_words" = freq_plt
      )
    
    # we also add the wordcloud if available
    if(word_cloud==TRUE){
      clouds <- list("Word_cloud"=clouds)
      output <- append(output,clouds)
      }
    
    # changing the name of the object based on the option given
    names(output)[1] <-
      `if`(lemma,"Remove_stopwords_make_lemma","Remove_stopwords")

    return(output)
  }

EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE)


# TF approach
pros_tf <- EDA_handler(bank_reviews, employer_pros, lemma=TRUE)
cons_tf <- EDA_handler(bank_reviews, employer_cons, lemma=TRUE)


# TF_IDF approach
pros_tf_idf <- EDA_handler(bank_reviews, employer_pros, n_top = 5,TF_grouped= TRUE, TF_IDF = TRUE)
cons_tf_idf <- EDA_handler(bank_reviews, employer_cons, n_top = 1,TF_grouped= TRUE, TF_IDF = TRUE)

# The grouped and specific to each document --> Only one word is shown which is quite strange and requires more investigation
pros_grouped <- EDA_handler(bank_reviews, employer_pros, n_top = 1, TF_grouped= TRUE)

# generating word cloud, note that you always get `null` in the output which
# is because based on base graphics and the side effect of drawing to the current graphics device.
pros_cloudy <- EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = TRUE)

# generating aggregated wordcloud
pros_cloudy_aggregated <- EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE)

cons_cloudy_aggregated <- EDA_handler(bank_reviews, employer_cons, n_top = 1, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE)

# note: in the rmarkdown approach you may have a warning that management could not fit on the screen, this is normal and in case it occurred a picture has to be posted of this.
```

The ggplot represents the most frequent words per company as the grouping was not done on the review but the company itself.

Regarding the pros, most often the words `benefit` and `people` come up, it is also interesting to see that the words `culture`, and `life` are also there perhaps indicating that most people care about these values when trying to describe work positively.

We can see that the most cons were associated with the word `management`. This is followed by `employees`, `hour` and `time`. Furthermore we have the same `people` word in the cons meaning that we would have to put our analysis into context and use the valence sifters to see if they mention something good about people or bad about these people.

Also `quanteda` library could be used if needed..

```{r}
pros_corpus <- bank_reviews %>% select(company, review_id, employer_pros) %>% corpus(text_field = "employer_pros")
```

### Semantic Analysis
Sentiment analysis with the two dictionaries of `nrc` and `afinn`.

```{r}
# Dictionaries for categories
get_sentiments("nrc")

# Dictionaries by values
get_sentiments("afinn")

# Use the lemmatized words and join them with a dictionary of choice
dicti_senti <-
  function (a_tf_object,
            a_dictionary,
            a_plot = FALSE) {
    # Using the tf matrix and removing the stop words
    tf_no_stop <- {
      a_tf_object
    }[[1]] %>%
      rename(word = contains("word")) #pro or con word
    
    # nrc option
    if (a_dictionary == "nrc") {
      nrc_sentiment <-
        tf_no_stop %>% inner_join(get_sentiments("nrc"))
      if (a_plot == TRUE) {
        plt <-
          nrc_sentiment %>%
          group_by(company, sentiment) %>%
          summarize(n = n()) %>%
          mutate(freq = n / sum(n)) %>%
          ggplot(aes(x = sentiment, y = freq, fill = sentiment)) +
          geom_bar(stat = "identity", alpha = 0.8) +
          facet_wrap(~ company) +
          coord_flip()
      }
      # in case a plot was requested we return it otherwise we just return the
      # sentiment output
      out<-
        tryCatch(
        expr = {
          output_nrc <-
            list("nrc_sentiments" = nrc_sentiment, "plot_of_nrc" = plt)
          return(output_nrc)
        },
        error = function(e) {
          return(nrc_sentiment)
        }
      )
      return(out) #if this is not done, then it'll return a null 
      # when a plot is not used
    }
    # afinn option
    if (a_dictionary == "afinn") {
      afinn_sentiment <-
        tf_no_stop %>% inner_join(get_sentiments("afinn"))
      if (a_plot == TRUE) {
        aggregated_measure <-
          aggregate(value ~ company, data = afinn_sentiment, FUN = mean)
        plt <-
          aggregated_measure %>% 
          ggplot(aes(x = company, y = value, fill = company)) +
          geom_bar(stat = "identity") +
          coord_flip()
      }
      
      # applying the same logic as before
      tryCatch(
        expr = {
          output_afinn <-
            list("afinn_sentiments" = afinn_sentiment,
                 "plot_of_afinn" = plt,
                 "aggregated_measure"=aggregated_measure)
          return(output_afinn)
        },
        error = function(e) {
          return(afinn_sentiment)
        }
      )
    }
  }


# looking at nrc pros and cons separately
pos_nrc_sem <- dicti_senti(pros_tf, a_dictionary = "nrc", a_plot=TRUE)
neg_nrc_sem <- dicti_senti(cons_tf, a_dictionary = "nrc", a_plot=TRUE)

# we can also combine the two reviews and look at them in that way
make_wider <- function(a_tibble) {
  a_tibble$Remove_stopwords_make_lemma %>% pivot_longer(
    cols = contains("word"),
    names_to = "type_of_review",
    names_pattern = "employer_(\\D+)s",
    values_to = "word"
  )
}

nrc_combined <- bind_rows(make_wider(pros_tf), make_wider(cons_tf))

# applying it to the combined set of reviews
dicti_senti(list(nrc_combined), a_dictionary = "nrc", a_plot=TRUE)

# doing the same with the afinn dictionary 
pos_afinn_sem <- dicti_senti(pros_tf, a_dictionary = "afinn", a_plot=TRUE)
neg_afinn_sem <- dicti_senti(cons_tf, a_dictionary = "afinn", a_plot=TRUE)

# computing the total for afiin, we add positives to negative semantics. 
scores_compared <- pos_afinn_sem$aggregated_measure %>% 
  left_join(neg_afinn_sem$aggregated_measure, by = "company") %>% 
  mutate(afinn_senti_score = value.x+value.y) %>% 
  arrange(desc(afinn_senti_score)) %>%
  select(-c(2,3))
```

Some interpetaiton --> To be completed...the fact that the feelings for the banks are very close to one another for the `nrc` method are close to one another whereas for the 


Semantic analysis using valence shifters
```{r, message=FALSE}
# library(tidyverse)
# library(tidytext)
# library(readr)

# We need to create one vector/bank which contain all the review
# I create a function, however, it should be possible to do it using nest function (dplyr), but laziness

# We create a function that extract each review and aggregate them in one vector
merge.review.pro <- function(.Data, companyname) {
  .Data %>%
    filter(company == companyname) %>%
    select(employer_pros) %>%
    str_c(sep = " ", collapse = TRUE)
}

merge.review.con <- function(.Data, companyname) {
  .Data %>%
    filter(company == companyname) %>%
    select(employer_cons) %>%
    str_c(sep = " ", collapse = TRUE)
}

# We iterate this function for each bank
company.name <- unique(bank_reviews$company)
reviews.bank.pro <- c("", "", "", "", "")
reviews.bank.con <- c("", "", "", "", "")

for (i in 1:length(company.name)) {
  reviews.bank.pro[i] <-
    merge.review.pro(bank_reviews, company.name[i])
}

for (i in 1:length(company.name)) {
  reviews.bank.con[i] <-
    merge.review.con(bank_reviews, company.name[i])
}

# We merge the prosand cons together and prepare the corpus
reviews.bank <- paste(reviews.bank.pro, reviews.bank.con, sep = "")
review.cp <- corpus(reviews.bank)

# We tokenize and assign the sentiment dictionnary to every pertinent token
review.tk <-
  review.cp %>% quanteda::tokens(
    remove_numbers = TRUE,
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_separators = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_replace(pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(stopwords("english"))

review.sent <-
  tokens_lookup(review.tk, dictionary = data_dictionary_LSD2015) %>%
  dfm() %>%
  tidy()

# We replace the name of docs by the company name
company.df <- t(t(company.name))
review.sent$document <- rbind(company.df, company.df)

# We scale the data to have comparable results
review.sent.scale <- review.sent %>%
  group_by(document) %>%
  mutate (factor = 100 / sum(count)) %>%
  mutate(percent = (count * factor) / 100)

# We plot the results
review.sent %>% ggplot(aes(x = document, y = count, fill = term)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  ylab("") +
  labs(title = "Sentiment Analysis (absolute value)", fill = "")

# We plot the results
review.sent.scale %>% ggplot(aes(x = document, y = percent, fill = term)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("") +
  ylab("") +
  labs(title = "Sentiment Analysis (relative value)", fill = "")
```
We observe that the dictionnary recognize a larger proportion of positive words. However, JP.Morgan and HSBC employees seems to have expressed less sentiment in their reviews. To be able to compare results, we scale the sentiment analysis and observe that the proportion of negative/positive review for each bank seems equal, although we can notice a slight negative trend for DB.


### More advanced methods with valence shifters.

```{r}
# Here we add the scores of the reviews which is more custom to our purpose
val_shifter <- function (a_tibble){
  object_1 <- a_tibble %>%
    dplyr::mutate(pros = get_sentences(employer_pros)) %$%
    sentiment_by(pros, list(company, review_id))
  
  object_2<- a_tibble %>%
    dplyr::mutate(cons = get_sentences(employer_cons)) %$%
    sentiment_by(cons, list(company, review_id))
  
  overall_score <-object_1 %>% 
    left_join(object_2, by = c("company", "review_id")) %>%
    mutate(score_numerical_analysis = ave_sentiment.x + ave_sentiment.y) %>%
    select(1, 2, score_numerical_analysis)
  
  average_score <- overall_score %>% group_by(company) %>%
    summarize(valshifter_senti_score=mean(score_numerical_analysis))
  
  return(average_score)
  }

# Applying the function to the bank reviews
valence_score<-val_shifter(bank_reviews)

# calculating the actual score they have given to each company
actual_score <- 
  aggregate(employer_rating~company, data=bank_reviews, FUN=mean) %>% 
  arrange(desc(employer_rating))

# Comparing the score of valence shifter vs the basic numerical sentiment
actual_score %>% 
  left_join(valence_score,by = "company") %>% 
  left_join(scores_compared,by = "company") %>%
  rename(actual_employer_rating = employer_rating) %>% 
  arrange(desc(2)) %>% kable_maker()
```

We get a score that 4/5 times the sentiment corresponds to the actual score, and more importantly, it highlights the differences in the scores with Morgan Stanley drastically being in lead.

### Job positions analysis
```{r, message=FALSE}

# We select and clean variable employee_role
clean.bank.review <-
  separate(
    data = bank_reviews,
    col = employee_role,
    into = c(NA, "employee_role"),
    sep = "\\-"
  )

jobs.corpus <- corpus(x = clean.bank.review ,
                      text_field = c("employee_role"))

jobs.tokens <- quanteda::tokens(
  jobs.corpus,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  what = "sentence"
)

jobs.dfm <- dfm(jobs.tokens) # create dfm
jobs.dfm$company <- bank_reviews$company
jobs.dfm <- dfm_group(jobs.dfm, groups = "company") %>% 
  dfm_sort(decreasing=TRUE)

# We filter the unapropriate terms
to.be.removed <- c(" anonymous employee",
          " teller ii",
          " teller i" ,
          " vice president",
          " assistant vice president",
          " director",
          " associate director"
          )
jobs.dfm <- jobs.dfm %>% dfm_remove(to.be.removed)
jobs.dfm <- jobs.dfm[,1:50] #sort dfm by only keeping most used term for better display

# We display position frequency by bank

jobs.freq <- textstat_frequency(jobs.dfm,groups = "company")
index.jobs <- jobs.freq 

jobs.freq %>%
  arrange(desc(frequency)) %>%
  filter(rank<=5) %>%
  ggplot(aes(x=feature, y=frequency)) + 
  geom_col() + 
  coord_flip() +
  facet_wrap(~group, ncol = 3) +
  labs(title="Top 5 most frequent position for each bank") + xlab("") +ylab("")

# Representation of the job position on the biplot
tmod <- textmodel_lsa(jobs.dfm, nd = 3)

biplot(
  y = tmod$docs[, 2:3],
  x = tmod$features[, 2:3],
  col = c("grey", "red"),
  xlab = "Dim 2",
  ylab = "Dim 3",
  cex=0.8
)

```
We have also explored the job positions to potentially identify patterns. After removing unaccurate job-title, we observe on the top 5 most frequent position per bank that some position seems more specific for one's bank. For instance, UBS seems to hire a larger number of interns for its operations.

To have a more interpretable insight and using LSA techniques, we manage to biplot in 2 dimensions the top 50 job positions. Interestingly, JP Morgan, HSBC and UBS seem to have a similar job structure. On the other, TD offers more position related to representant (customer oriented), whereas DB hire more analysts.

### Compare the reviews in terms of lexical diversity
```{r}

#corpus for the pros
reviews.corpus.pro <- corpus(x = bank_reviews,
                             text_field = c("employer_pros"))

### tokenization
library(lavaan)

#Romain: no need for word1 argument, as we are not concerned by symbols or whatever.
reviews.tokens.pro <- quanteda::tokens(reviews.corpus.pro, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        what="word")

# We decide to remove words that won't bring additional information, such as the name of the bank, etc.
to.be.removed <- c("ubs","jp","chase","td","morgan","hsbc","deutsche","db","jpmc","j.p","jpmorgan")

#Use of lemmitization
reviews.tokens.pro <- tokens_tolower(reviews.tokens.pro) %>% tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>% tokens_remove(c(stopwords("english"),to.be.removed))

reviews.pro.dfm <- dfm(reviews.tokens.pro)
reviews.company <-bank_reviews$company
reviews.pro.dfm$company <-reviews.company
reviews.bank.pro.dfm<-dfm_group(reviews.pro.dfm,groups = "company")

#frequency per terms
reviews.bank.pro.freq <- textstat_frequency(reviews.bank.pro.dfm,groups = "company")

#tf_idf
reviews.bank.pro.dfm.tfidf<-dfm_tfidf(reviews.bank.pro.dfm)

textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "I") %>%
  ggplot(aes(x = reorder(document, I), y = I)) +
  geom_point() +
  coord_flip() +
  xlab("Text") +
  ylab("Yule's index")

textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "TTR") %>%
  ggplot(aes(x = reorder(document, TTR), y = TTR)) +
  geom_point() +
  coord_flip() +
  xlab("Text") +
  ylab("TTR's index")


textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "MATTR") %>%
  ggplot(aes(x = reorder(document, MATTR), y = MATTR)) +
  geom_point() +
  coord_flip() +
  xlab("Text") +
  ylab("MATTR's index")
```


