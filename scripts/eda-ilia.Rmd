---
title: "EDA-Ilia"
output: html_document
---

```{r setup, include=FALSE}
# Loading packages and knitr options
source(here::here("scripts/setup.R"))

# Importing the data
bank_reviews <- read_csv(here::here("data/Bank_reviews_processed.csv"))

```

## Definitions

`Corpus` = All reviews on GlassDoor for five major banks
`Text` = Reviews for each bank --> Used this also as the `Document`
`Tokens` = Words in each review

### Data structure

```{r}
bank_reviews %>% glimpse
```

The dataset consists of `r length(unique(bank_reviews$company))` banks which are `r unique(bank_reviews$company) %>% paste(.,collapse = ", ")`.

We can summarize the information for each bank.

NOTE TO ilia: Replace the function below with an across as summarize_if is deprecated.

Checking for NAs and duplicates.

```{r}
freq.na(bank_reviews)

sum(duplicated(bank_reviews$review_id)) #The output shows that 429 reviews are duplicated, let's check them

# All the row's who values are duplicated
bank_reviews %>%
  filter(duplicated(review_id))

# These are all the duplicated values
bank_reviews %>% group_by(review_id) %>% filter(n()>1) %>% ungroup()

# We look at a specific example
bank_reviews %>% filter(review_id == "empReview_38099527")

# We will remove these rows
bank_reviews %<>%
  filter(!duplicated(review_id))

# We confirm that the duplicates are no longer there
bank_reviews %>% filter(review_id == "empReview_38099527")
```


```{r}
# General overview
bank_reviews %>%
  select(company, employer_rating:senior_management) %>%
  group_by(company) %>%
  add_count() %>%
  summarize_if(is.numeric,
               c(
                 "mean" = function(x)
                   mean(x, na.rm = TRUE),
                 "median" = function(x)
                   median(x, na.rm = TRUE)
               )) %>%
  relocate(company, n_median) %>%
  rename("number of reviews" = n_median) %>%
  kable_maker()

# An alternative shorter approach
# bank_reviews %>% 
#   dplyr::count(company, name = "number of reviews") %>% 
#   kable_maker()

# calculating the mean number of words
mean_words_cal <- function(a_word_column) {
  mean(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]"
  ), length))
}

median_words_cal <- function(a_word_column) {
  median(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]"
  ), length))
}

quantile_fun <- function(a_word_column,a_quantile) {
  quantile(sapply(strsplit(
    as.character(a_word_column), "[[:space:]]"
  ), length), a_quantile, na.rm = TRUE)
}

# applying the function to the two word columns
bank_reviews %>% group_by(company) %>%
  summarize(across(
    c(employer_pros, employer_cons),
    c(
      "1st_Qu." = function(x)quantile_fun(x, 0.25),
      "Median" = median_words_cal,
      "Mean" = mean_words_cal,
      "3rd_Qu." = function(x) quantile_fun(x, 0.75)
    )
  )) %>%
  gather(metric, val, 2:ncol(.)) %>%
  spread(company, val) %>% 
  kable_maker()

```

## EDA
### General explorations
A general EDA for the reviews Note that here we treat each company as a document.

```{r}
# creating function to take out the pros and cons of the reviews
EDA_handler <-
  function (a_tibble,
            text_column,
            n_top = 20,
            TF_grouped = FALSE,
            TF_IDF = FALSE,
            word_cloud = FALSE,
            aggregate_cloud = FALSE,
            lemma=FALSE) {
    # creating the new column name
    new_col <- paste0(quo_name(enquo(text_column)), "_word")
    # tokenization of the reviews
    review_token <- tidytext::unnest_tokens(
      a_tibble,
      output = "word",
      input = {{text_column}},
      to_lower = TRUE,
      strip_punct = TRUE,
      strip_numeric = TRUE) %>%
      # if the lemmatization option is activated
      `if`(lemma,{.} %>% mutate(word = lemmatize_strings(word)),.) %>%
      # removing the stop words
      anti_join(., stop_words, by = "word") %>%
      rename(., !!(new_col) := "word")
    # looking at the most frequent words
    freq <- review_token %>%
      group_by(company) %>%   # grouping is also possible also with review_id
      count("{{text_column}}_word" := get(new_col), sort = TRUE) %>%
      ungroup()
    # computing the TF-IDF if the option is set as TRUE
    if (TF_IDF == TRUE) {
      reviews_TF_IDF <- tidytext::bind_tf_idf(
        tbl = freq,
        term = !!(new_col),
        document = company,
        n = n
      )
    }
    # indexing for the top n words and also if it was a TFI-IDF then use that
    # for the index instead of the the typical frequency table
    index <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      `if`(TF_grouped, {.} %>% group_by(company), .) %>%
      top_n(n_top)
    # creating a function to be used for the ggplot aesthetics
    fix_lables <- function (a_column) {
      stri_replace_all_fixed(a_column, c("_", "s"), c(" ", ""), vectorize_all = FALSE)
    }
    # designing a ggplot for the most frequent words
    freq_plt <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      filter(get(new_col) %in% (index %>% pull(get(new_col)))) %>%
      ggplot(aes(x = get(new_col), y = `if`(TF_IDF, tf_idf, n), fill= get(new_col))) + 
      geom_col() + 
      coord_flip() + 
      facet_grid( ~ company) + 
      theme_light() +
      # another alternative for the wrap --> facet_wrap(~company, ncol = 2)
      labs(
        title = paste0("Most frequent ",
                       fix_lables(new_col),
                       "s per bank"),
        y = `if`(TF_IDF, "TF_IDF", NULL),
        x = fix_lables(new_col)
      )
    
    # creating wordcloud if the option is set equal to TRUE
    if (word_cloud == TRUE) {
      set.seed(1234)
      # dev.new(width = 1000, height = 1000, unit = "px")
      #many warnings are generated on this which is normal
      if (aggregate_cloud == TRUE) {
        freq_agg <- aggregate(as.formula(paste("n", "~", new_col, sep= " "))
                              , FUN = sum, data=freq)
      }
      clouds <-
        wordcloud::wordcloud(
          words = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(!!new_col)),
          freq = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(n)),
          min.freq = 2,
          max.words = 10,
          random.order = FALSE,
          random.color = FALSE,
          colors = brewer.pal(8, "Dark2"))
    }
    
    # generating the output
    output <-
      list(
        review_token,
        # "Remove_stopwords" = review_token,
        "Most_frequent_words" = freq,
        # if(exists("clouds")){"Word_cloud" = clouds},
        "Plotting_most_frequent_words" = freq_plt
      )
    
    # we also add the wordcloud if available
    if(word_cloud==TRUE){
      clouds <- list("Word_cloud"=clouds)
      output <- append(output,clouds)
      }
    
    # changing the name of the object based on the option given
    names(output)[1] <-
      `if`(lemma,"Remove_stopwords_make_lemma","Remove_stopwords")

    return(output)
  }

EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE)


# TF approach
pros_tf <- EDA_handler(bank_reviews, employer_pros, lemma=TRUE)
cons_tf <- EDA_handler(bank_reviews, employer_cons, lemma=TRUE)


# TF_IDF approach
pros_tf_idf <- EDA_handler(bank_reviews, employer_pros, n_top = 5,TF_grouped= TRUE, TF_IDF = TRUE)
cons_tf_idf <- EDA_handler(bank_reviews, employer_cons, n_top = 1,TF_grouped= TRUE, TF_IDF = TRUE)

# The grouped and specific to each document --> Only one word is shown which is quite strange and requires more investigation
pros_grouped <- EDA_handler(bank_reviews, employer_pros, n_top = 1, TF_grouped= TRUE)

# generating word cloud, note that you always get `null` in the output which
# is because based on base graphics and the side effect of drawing to the current graphics device.
pros_cloudy <- EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = TRUE)

# generating aggregated wordcloud
pros_cloudy_aggregated <- EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE)

cons_cloudy_aggregated <- EDA_handler(bank_reviews, employer_cons, n_top = 1, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE)

# note: in the rmarkdown approach you may have a warning that management could not fit on the screen, this is normal and in case it occurred a picture has to be posted of this.
```

The ggplot represents the most frequent words per company as the grouping was not done on the review but the company itself.

Regarding the pros, most often the words `benefit` and `people` come up, it is also interesting to see that the words `culture`, and `life` are also there perhaps indicating that most people care about these values when trying to describe work positively.

We can see that the most cons were associated with the word `management`. This is followed by `employees`, `hour` and `time`. Furthermore we have the same `people` word in the cons meaning that we would have to put our analysis into context and use the valence sifters to see if they mention something good about people or bad about these people.

Also `quanteda` library could be used if needed..

```{r}
pros_corpus <- bank_reviews %>% select(company, review_id, employer_pros) %>% corpus(text_field = "employer_pros")
```

### Semantic Analysis
Sentiment analysis with the two dictionaries of `nrc` and `afinn`.

```{r}
# Dictionaries for categories
get_sentiments("nrc")

# Dictionaries by values
get_sentiments("afinn")

# Use the lemmatized words and join them with a dictionary of choice
dicti_senti <-
  function (a_tf_object,
            a_dictionary,
            a_plot = FALSE) {
    # Using the tf matrix and removing the stop words
    tf_no_stop <- {
      a_tf_object
    }[[1]] %>%
      rename(word = contains("word")) #pro or con word
    
    # nrc option
    if (a_dictionary == "nrc") {
      nrc_sentiment <-
        tf_no_stop %>% inner_join(get_sentiments("nrc"))
      if (a_plot == TRUE) {
        plt <-
          nrc_sentiment %>%
          group_by(company, sentiment) %>%
          summarize(n = n()) %>%
          mutate(freq = n / sum(n)) %>%
          ggplot(aes(x = sentiment, y = freq, fill = sentiment)) +
          geom_bar(stat = "identity", alpha = 0.8) +
          facet_wrap(~ company) +
          coord_flip()
      }
      # in case a plot was requested we return it otherwise we just return the
      # sentiment output
      out<-
        tryCatch(
        expr = {
          output_nrc <-
            list("nrc_sentiments" = nrc_sentiment, "plot_of_nrc" = plt)
          return(output_nrc)
        },
        error = function(e) {
          return(nrc_sentiment)
        }
      )
      return(out) #if this is not done, then it'll return a null 
      # when a plot is not used
    }
    # afinn option
    if (a_dictionary == "afinn") {
      afinn_sentiment <-
        tf_no_stop %>% inner_join(get_sentiments("afinn"))
      if (a_plot == TRUE) {
        aggregated_measure <-
          aggregate(value ~ company, data = afinn_sentiment, FUN = mean)
        plt <-
          aggregated_measure %>% 
          ggplot(aes(x = company, y = value, fill = company)) +
          geom_bar(stat = "identity") +
          coord_flip()
      }
      # applying the same logic as before
      tryCatch(
        expr = {
          output_afinn <-
            list("afinn_sentiments" = afinn_sentiment,
                 "plot_of_afinn" = plt,
                 "aggregated_measure"=aggregated_measure)
          return(output_afinn)
        },
        error = function(e) {
          return(afinn_sentiment)
        }
      )
    }
  }


# looking at nrc pros and cons separately
pos_nrc_sem <- dicti_senti(pros_tf, a_dictionary = "nrc", a_plot=TRUE)
neg_nrc_sem <- dicti_senti(cons_tf, a_dictionary = "nrc", a_plot=TRUE)

# we can also combine the two reviews and look at them in that way
make_wider <- function(a_tibble) {
  a_tibble$Remove_stopwords_make_lemma %>% pivot_longer(
    cols = contains("word"),
    names_to = "type_of_review",
    names_pattern = "employer_(\\D+)s",
    values_to = "word"
  )
}

nrc_combined <- bind_rows(make_wider(pros_tf), make_wider(cons_tf))

# applying it to the combined set of reviews
dicti_senti(list(nrc_combined), a_dictionary = "nrc", a_plot=TRUE)

# doing the same with the afinn dictionary 
pos_afinn_sem <- dicti_senti(pros_tf, a_dictionary = "afinn", a_plot=TRUE)
neg_afinn_sem <- dicti_senti(cons_tf, a_dictionary = "afinn", a_plot=TRUE)

# computing the total for afiin, we add positives to negative semantics. 
scores_compared <- pos_afinn_sem$aggregated_measure %>% 
  left_join(neg_afinn_sem$aggregated_measure, by = "company") %>% 
  mutate(afinn_senti_score = value.x+value.y) %>% 
  arrange(desc(afinn_senti_score)) %>%
  select(-c(2,3))
```

Some interpetaiton --> To be completed...the fact that the feelings for the banks are very close to one another for the `nrc` method are close to one another whereas for the 

More advanced methods with valence shifters.

```{r}
# Here we add the scores of the reviews which is more custom to our purpose
val_shifter <- function (a_tibble){
  object_1 <- a_tibble %>%
    dplyr::mutate(pros = get_sentences(employer_pros)) %$%
    sentiment_by(pros, list(company, review_id))
  
  object_2<- a_tibble %>%
    dplyr::mutate(cons = get_sentences(employer_cons)) %$%
    sentiment_by(cons, list(company, review_id))
  
  overall_score <-object_1 %>% 
    left_join(object_2, by = c("company", "review_id")) %>%
    mutate(score_numerical_analysis = ave_sentiment.x + ave_sentiment.y) %>%
    select(1, 2, score_numerical_analysis)
  
  average_score <- overall_score %>% group_by(company) %>%
    summarize(valshifter_senti_score=mean(score_numerical_analysis))
  
  return(average_score)
  }

# Applying the function to the bank reviews
valence_score<-val_shifter(bank_reviews)

# calculating the actual score they have given to each company
actual_score <- 
  aggregate(employer_rating~company, data=bank_reviews, FUN=mean) %>% 
  arrange(desc(employer_rating))

# Comparing the score of valence shifter vs the basic numerical sentiment
actual_score %>% 
  left_join(valence_score,by = "company") %>% 
  left_join(scores_compared,by = "company") %>%
  rename(actual_employer_rating = employer_rating) %>% 
  arrange(desc(2)) %>% kable_maker()
```

We get a score that 4/5 times the sentiment corresponds to the actual score, and more importantly, it highlights the differences in the scores with Morgan Stanley drastically being in lead.






