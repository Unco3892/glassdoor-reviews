---
title: "Embedding-JDL"
output: html_document
---


```{r setup, include=FALSE}
# Loading packages and knitr options
source(here::here("scripts/setup.R"))

library(lexicon)
library(ngram)

# Importing the data
bank_reviews <- read_csv(here::here("data/Bank_reviews_processed.csv"))

# Adding word count column for pos
bank_reviews$wcount.pos <- unlist(lapply(bank_reviews$employer_cons, wordcount))

# Keeping only review with more than 5 words
bank_reviews_clean <- bank_reviews %>%
  filter(wcount.pos > 5)

# Corpus creation for pros
corpus.pro <- corpus(x = bank_reviews_clean,
                             text_field = c("employer_pros"))
```


```{r setup, include=FALSE}
# UBS
corpus.pro.ubs <- corpus_subset(corpus.pro, company == "UBS")


tk.pro.ubs <- tokens(corpus.pro.ubs, remove_numbers=TRUE, remove_punct=TRUE, 
                 remove_symbols=TRUE, remove_separators=TRUE)


tk.pro.ubs <- tk.pro.ubs %>% tokens_tolower() %>% tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>% tokens_remove(c("UBS","bank",stopwords("english")))
```

```{r}
# Creation of a COO
coo.pro.ubs <- fcm(tk.pro.ubs, context="window", window = 5, tri=FALSE) 

coo.pro.ubs
```

```{r}
library(text2vec)
p <- 2 # word embedding dimension
pro.ubs.glove <- GlobalVectors$new(rank = p, x_max = 10) # x_max is a needed technical option
pro.ubs.weC <- pro.ubs.glove$fit_transform(coo.pro.ubs) # central vectors; speech.glove$components contains the context vectors

pro.ubs.we <- t(pro.ubs.glove$components)+pro.ubs.weC # unique representation
```

```{r}
n.w <- apply(dfm(tk.pro.ubs),2,sum) ## compute the number of times ech term is used
index <- order(n.w, decreasing = TRUE)[1:50] # select the row-number corresponding to the 50 largest n.w

plot(pro.ubs.we[index,], type='n',  xlab="Dim 1", ylab="Dim 2")
text(x=pro.ubs.we[index,], labels=rownames(pro.ubs.we[index,]))
```
