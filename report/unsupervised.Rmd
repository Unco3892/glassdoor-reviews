<h1>
# Unsupervised Topic Analysis 
```{r, include=FALSE}
# Loading packages and knitr options
source(here::here("scripts/setup.R"))
```

```{r}
# Importing the data
bank_reviews <-
  read_csv(here::here("data/Bank_reviews_processed.csv")) %>%
  filter(!duplicated(review_id))

# Corpus creation for pros
corpus.pros <- corpus(x = bank_reviews,
                             text_field = c("employer_pros"))

# Corpus creation for pros
corpus.cons <- corpus(x = bank_reviews,
                             text_field = c("employer_cons"))
```

```{r,echo=FALSE}
#The output shows that 429 reviews are duplicated
# We will remove these rows
bank_reviews %<>%
  filter(!duplicated(review_id))
```

## Preprocessing

To start our analysis, we tokenized our data and removed the stopwords. To make a relevant analysis, we manually removed some words that, from our point of view, would create a bias in our analysis. Stopwords mainly include uninformative words such as quantifiers or company names which appear in every topic. The stopwords definition was an evolutive process. When we performed the unsupervised and superviseed analysis we changed our stopwords gradually. 
```{r, echo=TRUE}
tokens_to_remove <- c(
      "bank",#"ubs",#"jpmorgan",
      #"hsbc",#"td","deutsche" ,
      "#name","good","great",
      "work","company","can",
      "lot","much","get","job")
```


```{r}
# tokens for pros
tokens.pros <- quanteda::tokens(
  corpus.pros,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_separators=TRUE
) %>%
  tokens_tolower() %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(c(tokens_to_remove, stopwords("english")))

# tokens for cons
tokens.cons <- quanteda::tokens(
  corpus.cons,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_url = TRUE,
  remove_separators=TRUE
) %>%
  tokens_tolower() %>%
  tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>%
  tokens_remove(c(tokens_to_remove, stopwords("english")))
```

We selected the pros and cons reviews with at least five tokens because Glassdoor's users can not write a review with less than five words. 
```{r, echo=TRUE}
# Keeping reviews with more than a certain number of token for pros & cons
min.token <- 5

tokens.pros.clean <- tokens.pros %>%
  tokens_subset(ntoken(tokens.pros) > min.token & ntoken(tokens.cons) > min.token)

tokens.cons.clean <- tokens.cons %>%
  tokens_subset(ntoken(tokens.cons) > min.token & ntoken(tokens.pros) > min.token)
```

```{r}
# dfm pros
dfm.pros <- dfm(tokens.pros.clean)

# dfm cos
dfm.cons <- dfm(tokens.cons.clean)
```

## Latent dirichlet allocation Analysis

The LDA method displays each topic as a combination of words, and each document as a combination of topics. We created two functions, fors pros and cons, that displays our DFM in an LDA plot. 

## Dimensions testing

After several trials, we decided to fix the number of topics at 3 and the number of terms at 10 for pros and cons reviews. Having a small number of terms on the LDA plots allow us to better visualize the topics and adding more topics does not bring more insights.

```{r}
## convert quateda object to topicmodels object
K <- 3
pros.dtm <- convert(dfm.pros, to = "topicmodels")
lda.pros <- LDA(pros.dtm, k = K)

cons.dtm <- convert(dfm.cons, to = "topicmodels")
lda.cons <- LDA(cons.dtm, k = K)
```

###Topic by word
Here we can see that for pros and cons, it is very difficult to distinguish which words are more specific to each topic because the three topics have a lot of common words. 

```{r}
beta.pros <- tidy(lda.pros, matrix = "beta")

beta.pros.top.terms <- beta.pros %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

beta.pros.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  xlab("") +
  labs(title="3 dimensions: Top 10 terms for PROS")
```


```{r}
beta.cons <- tidy(lda.cons, matrix = "beta")

beta.cons.top.terms <- beta.cons %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

beta.cons.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  xlab("") +
  labs(title="3 dimensions: Top 10 terms for CONS")
```


###Topic by Company 
Here, we wanted to see the probability of each text being part of a topic. To do so, we have attached each document with the company, then we have made the average gamma by company, which allowed us to see that no company is closer to a topic than another. In fact, each bank has a probability higher than 30% to be part of the three topics for pros and cons. 
```{r}
# Retriving the company
company <- data.frame(cbind(docnames(dfm.pros), docvars(tokens.pros.clean, "company")), stringsAsFactors = FALSE)

company <- company %>%
  rename(document = X1,
         company = X2)
```


```{r, warning=FALSE, message=FALSE}
gamma.pros <- tidy(lda.pros, matrix = "gamma")

gamma.pros %>%
  left_join(company) %>%
  group_by(company, topic) %>%
  summarize(gamma = mean(gamma)) %>%
  ggplot(aes(company, gamma, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic) +
  coord_flip() +
  scale_x_reordered() +
  xlab("")
```


```{r, warning=FALSE, message=FALSE}
gamma.cons <- tidy(lda.cons, matrix = "gamma")


gamma.cons %>%
  left_join(company) %>%
  group_by(company, topic) %>%
  summarize(gamma = mean(gamma)) %>%
  ggplot(aes(company, gamma, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic) +
  coord_flip() +
  scale_x_reordered() +
  xlab("")
```



