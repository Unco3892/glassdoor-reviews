<h1>

# Exploratory Analysis
A general EDA for the reviews. Note that here we treat each company as a document. Our dataset is then composed of **23,341** documents.
ilia: reviews and not documents here

```{r, include=FALSE}
# Loading packages and knitr options
source(here::here("scripts/setup.R"))
# Loading the reviews from the introduction part
load(here::here("data/cleaned-reviews.RData"))
```

## Tokenization
Text Mining results are largely impacted by the tokenization process. After testing different options, we decide to use the lemmatization as it is the most promising option. 

We create the function `EDA_handler`, that deals with every aspect of the tokenization. The tokens are split by word and we also remove the stopwords. We also do not want to take into consideration the case and lowercase the tokens.

## Wordclouds {.tabset .tabset-fade .tabset-pills}
```{r,message=FALSE}
# creating function to take out the pros and cons of the reviews
EDA_handler <-
  function (a_tibble,
            text_column,
            n_top = 20,
            TF_grouped = FALSE,
            TF_IDF = FALSE,
            word_cloud = FALSE,
            aggregate_cloud = FALSE,
            lemma=TRUE) {
    # creating the new column name
    new_col <- paste0(dplyr::quo_name(rlang::enquo(text_column)), "_word")
    # tokenization of the reviews
    review_token <- tidytext::unnest_tokens(
      a_tibble,
      output = "word",
      input = {{text_column}},
      to_lower = TRUE,
      strip_punct = TRUE,
      strip_numeric = TRUE) %>%
      # if the lemmatization option is activated
      `if`(lemma,{.} %>% mutate(word = lemmatize_strings(word)),.) %>%
      # removing the stop words
      anti_join(., stop_words, by = "word") %>%
      rename(., !!(new_col) := "word")
    # looking at the most frequent words
    freq <- review_token %>%
      group_by(company) %>%   # grouping is also possible also with review_id
      count("{{text_column}}_word" := get(new_col), sort = TRUE) %>%
      ungroup()
    # computing the TF-IDF if the option is set as TRUE
    if (TF_IDF == TRUE) {
      reviews_TF_IDF <- tidytext::bind_tf_idf(
        tbl = freq,
        term = !!(new_col),
        document = company,
        n = n
      )
    }
    # indexing for the top n words and also if it was a TFI-IDF then use that
    # for the index instead of the the typical frequency table
    index <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      `if`(TF_grouped, {.} %>% group_by(company), .) %>%
      top_n(n_top)
    # creating a function to be used for the ggplot aesthetics
    fix_lables <- function (a_column) {
      stri_replace_all_fixed(a_column, c("_", "s"), c(" ", ""), vectorize_all = FALSE)
    }
    # designing a ggplot for the most frequent words
    freq_plt <-
      `if`(TF_IDF, reviews_TF_IDF, freq) %>%
      filter(get(new_col) %in% (index %>% pull(get(new_col)))) %>%
      ggplot(aes(x = get(new_col), y = `if`(TF_IDF, tf_idf, n), fill= get(new_col))) + 
      geom_col() + 
      coord_flip() + 
      facet_grid( ~ company) + 
      theme_light() +
      theme(legend.position = "none") +
      # another alternative for the wrap --> facet_wrap(~company, ncol = 2)
      labs(
        title = paste0("Most frequent ",
                       fix_lables(new_col),
                       "s per bank"),
        y = "Number of word occurence per bank",
        x = fix_lables(new_col)
      )
    
    # creating wordcloud if the option is set equal to TRUE
    if (word_cloud == TRUE) {
      set.seed(1234)
      # dev.new(width = 1000, height = 1000, unit = "px")
      #many warnings are generated on this which is normal
      if (aggregate_cloud == TRUE) {
        freq_agg <- aggregate(as.formula(paste("n", "~", new_col, sep= " "))
                              , FUN = sum, data=freq)
      }
      clouds <-
        wordcloud::wordcloud(
          words = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(!!new_col)),
          freq = (`if`(aggregate_cloud,freq_agg, freq) %>% pull(n)),
          min.freq = 2,
          max.words = 10,
          random.order = FALSE,
          random.color = FALSE,
          colors = brewer.pal(8, "Dark2"))
    }
    
    # generating the output
    output <-
      list(
        review_token,
        # "Remove_stopwords" = review_token,
        "Most_frequent_words" = freq,
        # if(exists("clouds")){"Word_cloud" = clouds},
        "Plotting_most_frequent_words" = freq_plt
      )
    
    # we also add the wordcloud if available
    if(word_cloud==TRUE){
      clouds <- list("Word_cloud"=clouds)
      output <- append(output,clouds)
      }
    
    # changing the name of the object based on the option given
    names(output)[1] <-
      `if`(lemma,"Remove_stopwords_make_lemma","Remove_stopwords")

    return(output)
  }
```

```{r, echo=FALSE,message=FALSE}
# TF approach
pros_tf <- EDA_handler(bank_reviews, employer_pros)
cons_tf <- EDA_handler(bank_reviews, employer_cons)

# TF_IDF approach
pros_tf_idf <- EDA_handler(bank_reviews, employer_pros, n_top = 5,TF_grouped= TRUE, TF_IDF = TRUE)
cons_tf_idf <- EDA_handler(bank_reviews, employer_cons, n_top = 1,TF_grouped= TRUE, TF_IDF = TRUE)

# The grouped and specific to each document
# pros_grouped <- EDA_handler(bank_reviews, employer_pros, n_top = 1, TF_grouped= TRUE)

# generating word cloud, note that you always get `null` in the output which is wordcloud cannot bs saved as R objects and they are based on base graphics and the side effect of drawing to the current graphics device.
#pros_cloudy <- EDA_handler(bank_reviews, employer_pros, n_top = 1, word_cloud = TRUE)
```

### Wordcloud for Pros
First, generate the wordcloud for the most common words found in the `employer_pros` column.
```{r, fig.asp=0.5}
# generating aggregated wordcloud
pros_cloudy_aggregated <- EDA_handler(bank_reviews, employer_pros, n_top = 4, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE) # here we use n_top = 4 for the ggplot and it does not influence our wordcloud

# note: in the rmarkdown approach you may have a warning that management could not fit on the screen, this is normal and in case it occurred a picture has to be posted of this.
```

### Wordcloud for Cons
Next, we can do the same for the `employer_cons` column.

```{r, fig.asp=0.8}
cons_cloudy_aggregated <- EDA_handler(bank_reviews, employer_cons, n_top = 5, word_cloud = T, aggregate_cloud = TRUE, lemma=TRUE)
```

### Most frequent Pro words

The the below represents the most frequent words per company as the grouping was not done on the review but the company itself.

```{r,fig.asp=0.5}
pros_cloudy_aggregated$Plotting_most_frequent_words
```

### Most frequent Cons Words

```{r,fig.asp=0.5}
cons_cloudy_aggregated$Plotting_most_frequent_words
```

## {-}
Regarding the pros, most often the words `benefit` and `people` come up, it is also interesting to see that the words `culture`, and `life` are also there perhaps indicating that most people care about these values when trying to describe work positively.

We can see that the most cons were associated with the word `management`. This is followed by `employees`, `hour` and `time`. Furthermore we have the same `people` word in the cons meaning that we would have to put our analysis into context and use the valence sifters to see if they mention something good about people or bad about these people.

## Semantic Analysis
First we will do a sentiment analysis with the two dictionaries of `nrc` and `afinn`. Then, we will introduce valence shifters in the subsequent part.

### NRC & AFINN {.tabset .tabset-fade .tabset-pills}

```{r,message=FALSE}
# Dictionaries for categories
get_sentiments("nrc")

# Dictionaries by values
get_sentiments("afinn")

# Use the lemmatized words and join them with a dictionary of choice
dicti_senti <-
  function (a_tf_object,
            a_dictionary,
            a_plot = FALSE) {
    # Using the tf matrix and removing the stop words
    tf_no_stop <- {
      a_tf_object
    }[[1]] %>%
      rename(word = contains("word")) #pro or con word
    
    # nrc option
    if (a_dictionary == "nrc") {
      nrc_sentiment <-
        tf_no_stop %>% inner_join(get_sentiments("nrc"))
      if (a_plot == TRUE) {
        plt <-
          nrc_sentiment %>%
          group_by(company, sentiment) %>%
          summarize(n = n()) %>%
          mutate(freq = n / sum(n)) %>%
          ggplot(aes(x = sentiment, y = freq, fill = sentiment)) +
          geom_bar(stat = "identity", alpha = 0.8) +
          facet_wrap(~ company) +
          coord_flip() +
          xlab("") +
          labs(title="PRO/CON: Sentiment analysis using nrc dictionnary")
      }
      # in case a plot was requested we return it otherwise we just return the
      # sentiment output
      out<-
        tryCatch(
        expr = {
          output_nrc <-
            list("nrc_sentiments" = nrc_sentiment, "plot_of_nrc" = plt)
          return(output_nrc)
        },
        error = function(e) {
          return(nrc_sentiment)
        }
      )
      return(out) #if this is not done, then it'll return a null 
      # when a plot is not used
    }
    # afinn option
    if (a_dictionary == "afinn") {
      afinn_sentiment <-
        tf_no_stop %>% inner_join(get_sentiments("afinn"))
      if (a_plot == TRUE) {
        aggregated_measure <-
          aggregate(value ~ company, data = afinn_sentiment, FUN = mean)
        plt <-
          aggregated_measure %>% 
          ggplot(aes(x = company, y = value, fill = company)) +
          geom_bar(stat = "identity") +
          coord_flip() +
          ylab("freq") +
          xlab("") +
          labs(title="PRO/CON: Sentiment analysis using Afinn dictionnary",fill="Bank name")
          
      }
      
      # applying the same logic as before
      tryCatch(
        expr = {
          output_afinn <-
            list("afinn_sentiments" = afinn_sentiment,
                 "plot_of_afinn" = plt,
                 "aggregated_measure"=aggregated_measure)
          return(output_afinn)
        },
        error = function(e) {
          return(afinn_sentiment)
        }
      )
    }
  }


# looking at nrc pros and cons separately
pos_nrc_sem <- dicti_senti(pros_tf, a_dictionary = "nrc", a_plot=TRUE)
neg_nrc_sem <- dicti_senti(cons_tf, a_dictionary = "nrc", a_plot=TRUE)

# we can also combine the two reviews and look at them in that way
make_wider <- function(a_tibble) {
  a_tibble$Remove_stopwords_make_lemma %>% pivot_longer(
    cols = contains("word"),
    names_to = "type_of_review",
    names_pattern = "employer_(\\D+)s",
    values_to = "word"
  )
}

nrc_combined <- bind_rows(make_wider(pros_tf), make_wider(cons_tf))

# applying it to the combined set of reviews
a<-dicti_senti(list(nrc_combined), a_dictionary = "nrc", a_plot=TRUE)
b<-dicti_senti(list(nrc_combined), a_dictionary = "afinn", a_plot=TRUE)


# doing the same with the afinn dictionary 
pos_afinn_sem <- dicti_senti(pros_tf, a_dictionary = "afinn", a_plot=TRUE)
neg_afinn_sem <- dicti_senti(cons_tf, a_dictionary = "afinn", a_plot=TRUE)

# computing the total for afiin, we add positives to negative semantics. 
scores_compared <- pos_afinn_sem$aggregated_measure %>% 
  left_join(neg_afinn_sem$aggregated_measure, by = "company") %>% 
  mutate(afinn_senti_score = value.x+value.y) %>% 
  arrange(desc(afinn_senti_score)) %>%
  select(-c(2,3))
```
<br />

#### Plot of NRC dictionary
```{r}
a$plot_of_nrc
```
The feelings for the banks are very close to one another for the `nrc` method. Unfortunately, we won't be able to use nrc dictionnary for classification.
<br />

#### Plot of AFINN dictionary

```{r}
b$plot_of_afinn
```

<br />

Using Afinn dictionnary, the sentiment analysis seems more promising. It really emphasize the importance of choosing the correct dictionary. The Afinn contains over 3,300+ words with a polarity score associated with each word. 
<br />

### More advanced methods with valence shifters.

```{r}
# Here we add the scores of the reviews which is more custom to our purpose
val_shifter <- function (a_tibble){
  object_1 <- a_tibble %>%
    dplyr::mutate(pros = get_sentences(employer_pros)) %$%
    sentiment_by(pros, list(company, review_id))
  
  object_2<- a_tibble %>%
    dplyr::mutate(cons = get_sentences(employer_cons)) %$%
    sentiment_by(cons, list(company, review_id))
  
  overall_score <-object_1 %>% 
    left_join(object_2, by = c("company", "review_id")) %>%
    mutate(score_numerical_analysis = ave_sentiment.x + ave_sentiment.y) %>%
    select(1, 2, score_numerical_analysis)
  
  average_score <- overall_score %>% group_by(company) %>%
    summarize(valshifter_senti_score=mean(score_numerical_analysis))
  
  return(average_score)
  }

# Applying the function to the bank reviews
valence_score<-val_shifter(bank_reviews)

# calculating the actual score they have given to each company
actual_score <- 
  aggregate(employer_rating~company, data=bank_reviews, FUN=mean) %>% 
  arrange(desc(employer_rating))

# Comparing the score of valence shifter vs the basic numerical sentiment
actual_score %>% 
  left_join(valence_score,by = "company") %>% 
  left_join(scores_compared,by = "company") %>%
  rename(actual_employer_rating = employer_rating) %>% 
  arrange(desc(2)) %>% kable_maker(caption="Employer rating: Comparison with 2 sentiment analysis methods",
                                      col.names=c("Bank name",
                                                  "Actual employer rating",
                                                  "Valence shifter score",
                                                  "Afinn sentiment score"))
```

We get a score that 4/5 times the sentiment corresponds to the actual score. More importantly, it highlights the differences in the scores with Morgan Stanley drastically being in lead. Consequently, this new approach of valence shifter score is a good candidate for classification. We choose to integrate it for the supervised learning part.

## Job positions analysis

In order to have the most accurate classifier, we need to have a look at every variable available. During the scraping process, we have also managed to extract the job position. It makes sense to have a look at it to see if the employee role has an impact on the company review score.

```{r fig4, out.width = "90%"}
# We select and clean variable employee_role
clean.bank.review <-
  separate(
    data = bank_reviews,
    col = employee_role,
    into = c(NA, "employee_role"),
    sep = "\\-"
  )

jobs.corpus <- corpus(x = clean.bank.review ,
                      text_field = c("employee_role"))

jobs.tokens <- quanteda::tokens(
  jobs.corpus,
  remove_punct = TRUE,
  remove_symbols = TRUE,
  what = "sentence"
)

jobs.dfm <- dfm(jobs.tokens) # create dfm
jobs.dfm$company <- bank_reviews$company
jobs.dfm <- dfm_group(jobs.dfm, groups = "company") %>% 
  dfm_sort(decreasing=TRUE)

# We filter the unapropriate terms
to.be.removed <- c(" anonymous employee",
          " teller ii",
          " teller i" ,
          " vice president",
          " assistant vice president",
          " director",
          " associate director"
          )
jobs.dfm <- jobs.dfm %>% dfm_remove(to.be.removed)
jobs.dfm <- jobs.dfm[,1:50] #sort dfm by only keeping most used term for better display

# We display position frequency by bank

jobs.freq <- textstat_frequency(jobs.dfm,groups = "company")
index.jobs <- jobs.freq 

jobs.freq %>%
  arrange(desc(frequency)) %>%
  filter(rank<=5) %>%
  ggplot(aes(x=feature, y=frequency)) + 
  geom_col() + 
  coord_flip() +
  facet_wrap(~group, ncol = 3) +
  labs(title="Top 5 most frequent position for each bank") + xlab("") +ylab("")


```
We have also explored the job positions to potentially identify patterns. After removing inaccurate job-title, we observe on the top 5 most frequent position per bank that some position seems more specific for one's bank. For instance, UBS seems to hire a larger number of interns for its operations.

ilia: for one's bank or "to each bank?"

```{r fig5, out.width = "70%",fig.asp=1.1}

# Representation of the job position on the biplot
tmod <- textmodel_lsa(jobs.dfm, nd = 3)

biplot(
  y = tmod$docs[, 2:3],
  x = tmod$features[, 2:3],
  col = c("grey", "red"),
  xlab = "Dim 2",
  ylab = "Dim 3",
  cex=0.8
)
```


To have a more interpretable insight and using LSA techniques, we manage to create a biplot in 2 dimensions the top 50 job positions. Interestingly, JP Morgan, HSBC and UBS seem to have a similar job structure. On the other, TD offers more position related to representants (customer oriented), whereas DB hire more analysts.

## Compare the reviews in terms of lexical diversity

Lexical diversity can also be an interesting tool for classification. However, it seems unlikely to have a usable result here. The only explanation would be that maybe some specific banks hire many more non-native English speaker, whose linguistic abilities in exercising this langauge may be lower.

```{r fig6,fig.width=9,out.width = "90%"}
#corpus for the pros
reviews.corpus.pro <- corpus(x = bank_reviews,
                             text_field = c("employer_pros"))

### tokenization
library(lavaan)

#Romain: no need for word1 argument, as we are not concerned by symbols or whatever.
reviews.tokens.pro <- quanteda::tokens(reviews.corpus.pro, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        what="word")

# We decide to remove words that won't bring additional information, such as the name of the bank, etc.
to.be.removed <- c("ubs","jp","chase","td","morgan","hsbc","deutsche","db","jpmc","j.p","jpmorgan")

#Use of lemmitization
reviews.tokens.pro <- tokens_tolower(reviews.tokens.pro) %>% tokens_replace(pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) %>% tokens_remove(c(stopwords("english"),to.be.removed))

reviews.pro.dfm <- dfm(reviews.tokens.pro)
reviews.company <-bank_reviews$company
reviews.pro.dfm$company <-reviews.company
reviews.bank.pro.dfm<-dfm_group(reviews.pro.dfm,groups = "company")

#frequency per terms
reviews.bank.pro.freq <- textstat_frequency(reviews.bank.pro.dfm,groups = "company")

#tf_idf
reviews.bank.pro.dfm.tfidf<-dfm_tfidf(reviews.bank.pro.dfm)

a<-textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "I") %>%
  ggplot(aes(x = reorder(document, I), y = I)) +
  geom_point() +
  coord_flip() +
  xlab("") +
  ylab("Yule's index")

b<-textstat_lexdiv(reviews.bank.pro.dfm,
                measure = "TTR") %>%
  ggplot(aes(x = reorder(document, TTR), y = TTR)) +
  geom_point() +
  coord_flip() +
  xlab("") +
  ylab("TTR's index")

ggarrange(a,b,
          ncol=2,nrow=1)
```

We use both Yule's index and TTR's index to demonstrate that they are not candidate for classification. Both graphs tell very different stories, and we cannot correlate these results with the `employee_review`.
