<h1>
#Supervised Learning
In this supervised learning part, we applied machine learning methods to model our data in order to predict the rating of the review. To do so we proceed the following way: 

We applied a filter to remove the duplicated entries and also add sentiments function separately for pros and con which will be combined later. In addition, we also added the word count. We decided to not use the sentiment function as it gave some recycling issues. We used get_sentences to avoid warnings. After, using lematization with the function token_replace, we decided to use reviews containing at least 10 tokens. 

Given the fact that our motivation is to fit a model that can output accurate predictions, we used linear regression and random forest model with LSA and Word Embeding. Added to this, we have built the document frequency matrix bag of words and the TF-IFD which helped us to define the importance of each words for specific document. 

For supervised learning, we decided to apply a different strategy from the unsupervised part. In fact,instead of having 3 topics for pros and cons, we decided to mix the two and assign a different number of topics for each of the two categories (ex: we can have 3 topics for the pros and 20 topics for the cons) to achieve better predictions. More precisely, we created a matrix of what is the ideal number of topics for pros and cons.  In our case, we decided to use the RMSE to tell use which combination is the best. There is two ways to proceed in our case. The first one is to do the LSA for cons and pros and then use both of them in the model, the second is to mix pros and cons and do the LSA . 


```{r , include=FALSE}
# Loading packages and knitr options
source(here::here("scripts/setup.R"))


# Importing the data
bank_reviews <- read_csv(here::here("data/Bank_reviews_processed.csv"))
```

```{r, include=FALSE}

# Apply a filter to remove the duplicated entries and also add sentiments separately for pros and con which we will combine later. The word count was also added
bank_reviews %<>%
  filter(!duplicated(review_id)) %>%
  dplyr::mutate(
    pro_sen_score = sentimentr::sentiment_by(text.var = get_sentences(employer_pros))$ave_sentiment,
    # we did not use the sentiment function as it gave some recycling issues. We use get_sentences to avoid warnings but it is not necessary
    cons_sen_score = sentimentr::sentiment_by(get_sentences(employer_cons))$ave_sentiment,
    pros_w_count = str_count(employer_pros, '\\w+'),
    cons_w_count = str_count(employer_cons, '\\w+')
  )

# Corpus creation for pros
corpus.pros <- corpus(x = bank_reviews,
                      text_field = c("employer_pros"))

# Corpus creation for pros
corpus.cons <- corpus(x = bank_reviews,
                      text_field = c("employer_cons"))
```

```{r}
# tokens for pros
tokenize_reviews <- function(a_corpus) {
  tokens.pros <- quanteda::tokens(
    a_corpus,
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_url = TRUE
  ) %>%
    tokens_tolower() %>%
    tokens_remove(
      c(
        "bank",
        "ubs",
        "jpmorgan",
        "hsbc",
        "td",
        "deutsche" ,
        "#name",
        "good",
        "great",
        stopwords("english")
      )
    ) %>%
    tokens_replace(pattern = hash_lemmas$token, replacement = hash_lemmas$lemma)
}

# tokens for pros
tokens.pros <- tokenize_reviews(corpus.pros)

# tokens for cons
tokens.cons <- tokenize_reviews(corpus.cons)
```


```{r, echo=TRUE}
# Keeping reviews with more than a certain number of token for pros & cons
min.token <- 10

tokens.pros.clean <- tokens.pros %>%
  tokens_subset(ntoken(tokens.pros) > min.token & ntoken(tokens.cons) > min.token)

tokens.cons.clean <- tokens.cons %>%
  tokens_subset(ntoken(tokens.cons) > min.token & ntoken(tokens.pros) > min.token)
```


```{r}
# dfm pros
dfm.pros <- dfm(tokens.pros.clean)

# dfm cos
dfm.cons <- dfm(tokens.cons.clean)

# tfidf pros
tfidf.pros <- dfm_tfidf(dfm.pros)

# tfidf cons
tfidf.cons <- dfm_tfidf(dfm.cons)
```


```{r}
# Retrieving the ratings
y <- docvars(tokens.pros.clean, "employer_rating")

# Creating the training Set
set.seed(123)
index.tr <- sample(size=round(0.8*length(y)), x=c(1:length(y)), replace=FALSE)
```

## Document Term Matrix
The following table shows us that the lowest RMSE is obtained by using 50 topics for the pros and only 5 for the cons. The best result (RMSE= 0.913) is given by a linear model using DTM+LSA. 
```{r}
# Creating a result Dataframe
results_lsa_dtm <- expand.grid(
  "method" = c("LSA"),
  "input" = c("DTM"),
  "dimPos" = c(5, 20, 30, 50),
  "dimCons" = c(5, 20, 30, 50),
  "LM" = NA,
  "RF" = NA,
  # NA columns for our future calculations
  stringsAsFactors = FALSE
) %>%
  arrange(dimPos, dimCons)


# Compute the RMSE for each combination of #Topic for pros and cons
for (i in 1:nrow(results_lsa_dtm)){
  #LSA
  lsa.pros <- textmodel_lsa(dfm.pros, nd=results_lsa_dtm$dimPos[i])
  lsa.cons <- textmodel_lsa(dfm.cons, nd=results_lsa_dtm$dimCons[i])
  # df <- data.frame(Score=y, X1=lsa.pros$docs, X2=lsa.cons$docs)
  
  df <- data.frame(
    Score = y,
    topic.pros = lsa.pros$docs,
    topic.cons = lsa.cons$docs,
    total_sen_score = rowSums(docvars(tokens.pros.clean, c("pro_sen_score", "cons_sen_score")))
  )

    
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  
  set.seed(100)
  #LM
  reviews.lm <- lm(Score ~ .,
                   data = df.tr)
  pred.lm <- predict(reviews.lm, df.te)
  pred.lm <- pmin(5, predict(reviews.lm, df.te))
  results_lsa_dtm$LM[i] <- sqrt(mean((pred.lm - df.te$Score) ^ 2)) 
  
  #RF
  reviews.rf <- ranger(Score ~ .,
                       data = df.tr)
  pred.rf <- predict(reviews.rf, df.te)$predictions
  results_lsa_dtm$RF[i] <- sqrt(mean((pred.rf - df.te$Score) ^ 2))
}


results_lsa_dtm %>%
  kable_maker(caption="Document-term frequency: RMSE for LM and RF",
                                      col.names=c("Method","Input",
                                                  "# of dimensions pros",
                                                  "# of dimensions cons",
                                                  "Linear model",
                                                  "Random forest"))
```

## TF-IDF
The following table shows us that the lowest RMSE is obtained by using 20 topics for the pros and only 5 for the cons. The best result (RMSE= 0.901) is given by a random forest using TF-IDF+LSA. 

```{r}
results_lsa_tfidf <- expand.grid(
  "method" = c("LSA"),
  "input" = c("TFIDF"),
  "dimPos" = c(5, 20, 30, 50),
  "dimCons" = c(5, 20, 30, 50),
  "LM" = NA,
  "RF" = NA,
  # NA columns for our future calculations
  stringsAsFactors = FALSE
) %>%
  arrange(dimPos, dimCons)

# Compute the RMSE for each combination of #Topic for pros and cons
for (i in 1:nrow(results_lsa_tfidf)){
  #LSA
  lsa.pros <- textmodel_lsa(tfidf.pros, nd=results_lsa_tfidf$dimPos[i])
  lsa.cons <- textmodel_lsa(tfidf.cons, nd=results_lsa_tfidf$dimCons[i])
  # df <- data.frame(Score=y, X1=lsa.pros$docs, X2=lsa.cons$docs)
  
  df <- data.frame(
  Score = y,
  topic.pros = lsa.pros$docs,
  topic.cons = lsa.cons$docs,
  total_sen_score = rowSums(docvars(
    tokens.pros.clean, c("pro_sen_score", "cons_sen_score"))),
    total_word_count = rowSums(log10(docvars(
        tokens.pros.clean, c("pros_w_count", "cons_w_count"))
  )))
  
  df.tr <- df[index.tr,]
  df.te <- df[-index.tr,]
  
  set.seed(100)
  #LM
  reviews.lm <- lm(Score ~ .,
                   data = df.tr)
  pred.lm <- predict(reviews.lm, df.te)
  pred.lm <- pmin(5, predict(reviews.lm, df.te))
  results_lsa_tfidf$LM[i] <- sqrt(mean((pred.lm - df.te$Score) ^ 2))

  #RF
  reviews.rf <- ranger(Score ~ .,
                       data = df.tr)
  pred.rf <- predict(reviews.rf, df.te)$predictions
  results_lsa_tfidf$RF[i] <- sqrt(mean((pred.rf - df.te$Score) ^ 2))
}

results_lsa_tfidf %>%
  kable_maker(caption="TF-IDF: RMSE for LM and RF",
                                      col.names=c("Method","Input",
                                                  "# of dimensions pros",
                                                  "# of dimensions cons",
                                                  "Linear model",
                                                  "Random forest"))  
```

##Combining the results
The following table shows us that the lowest RMSE is obtained by using 30 topics for the pros and 20 for the cons. The best result (RMSE= 0.900) is given by a random forest using TF-IDF+LSA, which gave us the best results among the 3. 
```{r}
# combining the results
results_lsa <- rbind(results_lsa_dtm, results_lsa_tfidf)

results_lsa <- results_lsa %>%
  gather(LM, RF, key = "learner", value = "RMSE")


bestmodel <- results_lsa %>%
  top_n(-1, RMSE)

# best models : 
results_lsa %>%
  arrange(RMSE) %>%
  kable_maker(caption="MIXED: RMSE for LM and RF",
                                      col.names=c("Method","Input",
                                                  "# of dimensions pros",
                                                  "# of dimensions cons",
                                                  "Linear model",
                                                  "Random forest")) 

# # worst models : 
# results_lsa %>%
#   top_n(10, RMSE) %>%
#   arrange(RMSE)
```


# Best model (TF-IDF + LSA)
Here we added to our best model valence shifter (sentiment function mentioned above) and the length of the reviews. We can see that despite the addition of the new element, it does not improve our RMSE (0.991) and that the results obtained previously are all better. However, we can see that our model does quit well when it has to predict twos and threes, which could be explain by the fact that there a more reviews which have been rated with a two or a three. 


```{r, echo=TRUE}
lsa.pros <- textmodel_lsa(tfidf.pros, nd = bestmodel$dimPos)
# lsa.cons <- textmodel_lsa(tfidf.cons, nd = 20)
# Even better for the cons to be 30
lsa.cons <- textmodel_lsa(tfidf.cons, nd =  bestmodel$dimCons)
df <- data.frame(
  Score = y,
  pros = lsa.pros$docs,
  cons = lsa.cons$docs,
  # Note that in the code below, we can use either tokens.pros.clean or tokens.cons.clean as they both contain the same sentiment and word count columns
  total_sen_score = rowSums(docvars(
    tokens.pros.clean, c("pro_sen_score", "cons_sen_score")
  ))
)

df.tr <- df[index.tr, ]
df.te <- df[-index.tr, ]

#RF
set.seed(100)
reviews.rf <- ranger(Score ~ .,
                     data = df.tr,
                     importance = "impurity")

pred.rf <- predict(reviews.rf, df.te)$predictions

rmse <- sqrt(mean((pred.rf - df.te$Score)^2))

```


```{r}
plot(pred.rf ~ df.te$Score, ylab="Predictions", xlab="Observed", pch=20)
```

### Variable importance

Here we can see that the lenght of pros and cons are important and that the sentiment score helps predicting greatly. We can also observe that cons.3 seems to be an important topic. 
```{r}
imp <- data.frame(keyName=names(reviews.rf$variable.importance), value=reviews.rf$variable.importance, row.names=NULL)

imp %<>% mutate(type= case_when(str_detect(keyName, "pros") ~ "pros",
               str_detect(keyName, "cons") ~ "cons",
               !str_detect(keyName, "pros|cons") ~ "other"))
```

```{r}
imp %>%
  mutate(name = fct_reorder(keyName, value)) %>%
  ggplot(aes(x = name, y = value, fill = type)) +
  geom_bar(stat = "identity",
           alpha = .6,
           width = .4) +
  coord_flip() +
  xlab("") +
  ylab("impurity") +
  theme_bw() +
  scale_fill_manual(breaks = c("pros", "cons", "other"),
                       values=c("green", "red", "blue"))
```

In this table we are invistigating the words in cons.3. We can see that the 2 most important variables constituting cons.3 are people and team, which mean quite the same.  

```{r}
cons.3 <- data.frame(keyName = names(
  lsa.cons$features[, 3]),
  value = lsa.cons$features[, 3],
  row.names = NULL
)

cons.3 %>% mutate(abs.value = abs(value)) %>% top_n(15, abs.value) %>%
  arrange(desc(value)) %>%
  kable_maker(caption="The most important variable seems related to colleagues",
                                      col.names=c("Key words","Value",
                                                  "Absolute value"))
```

Now let's have a look at a pro topic. We can clearly see that pros.3 is related to customer relationship, with words such as cutomer, commerce, deposit, account and retail. 
```{r}
pros.3 <- data.frame(keyName = names(
  lsa.pros$features[, 3]),
  value = lsa.pros$features[, 3],
  row.names = NULL
)

pros.3 %>% mutate(abs.value = abs(value)) %>% top_n(15, abs.value) %>%
  arrange(desc(value)) %>%
  kable_maker(caption="Topic pro 3 - Related to customer relationship",
                                      col.names=c("Key words","Value",
                                                  "Absolute value"))
```

These results may suggest that, in general, employees are satisfied with the quality of the services provided by the company and that they are dissatisfied with internal management methods. 

## Word Embedding & Glove
To complete our supervised learning, we used the Word Embedding and GloVe embedding to model in a different way. First, we computed the RMSE for each combination of number of topic for pros and cons and then, while creating our data frame, we took the mean words instead of the sum as it worked better. Finally, we can see that the results are without appeal, GloVe is worse than LSA.

```{r}
# Tried windows 5,4,3 and 4 seems reasonable
tokens.pros.clean_fcm <- fcm(
  tokens.pros.clean,
  context = "window",
  count = "weighted",
  window = 4,
  weights = 1 / (1:4),
  tri = FALSE
)

# with embedding for cons
tokens.cons.clean_fcm <- fcm(
  tokens.cons.clean,
  context = "window",
  count = "weighted",
  window = 4,
  weights = 1 / (1:4),
  tri = FALSE
)
```


```{r}
# Result matrix
results_glove <- expand.grid(
  "method" = c("Glove"),
  "input" = c("FCM"),
  "dimPos" = c(5, 20, 50),
  "dimCons" = c(5, 20, 50),
  "learner" = c("RF"),
  "RMSE" = NA,
  # NA columns for our future calculations
  stringsAsFactors = FALSE
) %>%
  arrange(dimPos, dimCons)
```


```{r, results='hide',message=FALSE, echo=TRUE}
library(text2vec)
# Compute the RMSE for each combination of #Topic for pros and cons
for (i in 1:nrow(results_glove)) {
    # Giving dimensions to the embedding
    glove_pros <- GlobalVectors$new(rank = results_glove$dimPos[i], x_max = 1)
    glove_cons <- GlobalVectors$new(rank = results_glove$dimCons[i], x_max = 1)
    
    # fitting the model, with the targets in mind
    word_vectors_main_pro <-
      glove_pros$fit_transform(tokens.pros.clean_fcm, n_iter = 50)
    print(paste("Iteration for pros at:",results_glove$dimPos[i]))
    
    word_vectors_main_cons <-
      glove_cons$fit_transform(tokens.cons.clean_fcm, n_iter = 50)
    print(paste("Iteration for cons at:",results_glove$dimCons[i]))

    # Taking out the context of the model
    word_vectors_context_pros <- glove_pros$components
    word_vectors_context_cons <- glove_cons$components
    
    # Adding the target + transpose of the context words
    reviews_glove_pro <-
      word_vectors_main_pro + t(word_vectors_context_pros)
    reviews_glove_con <-
      word_vectors_main_cons + t(word_vectors_context_cons)
    
    # number of documents which is the same for both
    ndoc <- length(tokens.pros.clean)
    
    # looking at the centers of pros and cons
    centers_pros <-
      matrix(nr = ndoc, nc = results_glove$dimPos[i]) # document embedding matrix
    centers_cons <-
      matrix(nr = ndoc, nc = results_glove$dimCons[i]) # document embedding matrix
    
    for (f in 1:ndoc) {
      words_in_f_p <-
        reviews_glove_pro[tokens.pros.clean[[f]], , drop = FALSE]
      
      centers_pros[f, ] <- apply(words_in_f_p, 2, mean)
      
      words_in_f_c <-
        reviews_glove_con[tokens.cons.clean[[f]], , drop = FALSE]
      
      centers_cons[f, ] <- apply(words_in_f_c, 2, mean)
    }
    
    row.names(centers_pros) <- names(tokens.pros.clean)
    row.names(centers_cons) <- names(tokens.cons.clean)
    
    df <- data.frame(
      Score = y,
      X1 = centers_pros,
      X2 = centers_cons,
      total_sen_score = rowSums(docvars(
        tokens.pros.clean, c("pro_sen_score", "cons_sen_score")
      )),
      total_word_count = rowMeans(docvars(
        tokens.pros.clean, c("pros_w_count", "cons_w_count")
      ))) #here we take the mean words instead of the sum as it worked better
    
    df.tr <- df[index.tr, ]
    df.te <- df[-index.tr, ]

    #RF
    reviews.rf <- ranger(Score ~ .,
                         data = df.tr)
    pred.rf <- predict(reviews.rf, df.te)$predictions
    results_glove$RMSE[i] <- sqrt(mean((pred.rf - df.te$Score) ^ 2))
  }
```

```{r}
results_glove %>%
  arrange(RMSE) %>%
  kable_maker(caption="GLOVE: RMSE results",
                                      col.names=c("Method","Input",
                                                  "# of dimensions pros",
                                                  "# of dimensions cons",
                                                  "Learner",
                                                  "RMSE"))
```


