library(tidyverse)
source("R/scrape.R")
#> Loading libraries...
#> Sourcing functions...
# example urls, we'll go with Google
tesla_url <- "https://www.glassdoor.com/Reviews/Tesla-Reviews-E43129"
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079.htm"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
# filter for stuff we successfully extracted
reviews <- bind_rows(Filter(Negate(is.null), out), .id = "page")
reviews
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://de.glassdoor.ch/Bewertungen/Google-Bewertungen-E9079.htm?countryRedirect=true"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
# filter for stuff we successfully extracted
reviews <- bind_rows(Filter(Negate(is.null), out), .id = "page")
reviews
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://de.glassdoor.ch/Bewertungen/Google-Bewertungen-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
# filter for stuff we successfully extracted
reviews <- bind_rows(Filter(Negate(is.null), out), .id = "page")
reviews
library(gdscrapeR)
install.packages("devtools")
devtools::install_github("mguideng/gdscrapeR")
library(gdscrapeR)
install.packages("devtools")
library(gdscrapeR)
df <- get_reviews(companyNum = "E9079")
df
#### SCRAPE ####
# Packages
library(httr)  #get HTML document: GET()
library(xml2)  #convert to XML document: read_html()
library(rvest) #select & extract text from XML: html_nodes() & html_text() html_attr()
library(purrr) #iterate scraping and return data frame: map_df()
# Set URL (.com English canonical link)
baseurl <- "https://www.glassdoor.com/Reviews/Company-Reviews-"
companyNum <- "E40371"
sort <- ".htm?sort.sortType=RD&sort.ascending=true"
# How many total number of reviews? It will determine the maximum page results to iterate over.
totalReviews <- read_html(paste(baseurl, companyNum, sort, sep = "")) %>%
html_nodes(".tightVert.floatLt strong, .margRtSm.minor") %>%
html_text() %>%
sub("Found | reviews", "", .) %>%
sub(",", "", .) %>%
as.integer()
maxResults <- as.integer(ceiling(totalReviews/10))    #10 reviews per page, round up to whole number
# Create data frames for: Date, Summary, Rating, Title, Pros, Cons, Helpful
# 1. Date
date <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".date.subtle.small, .featuredFlag") %>%
html_text() %>% data.frame(rev.date = ., stringsAsFactors = F)
})
# 2. Summary
summary <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".reviewLink .summary:not([class*='toggleBodyOff'])") %>%
html_text() %>% data.frame(rev.sum = ., stringsAsFactors = F)
})
# 3. Rating
rating <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".gdStars.gdRatings.sm .rating .value-title") %>%
html_attr("title") %>% data.frame(rev.rating = ., stringsAsFactors = F)
})
# 4. Title
title <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes("span.authorInfo.tbl.hideHH") %>%
html_text() %>% data.frame(rev.title = ., stringsAsFactors = F)
})
# 5. Pros
pros <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".description .row:nth-child(1) .mainText:not([class*='toggleBodyOff'])") %>%
html_text() %>% data.frame(rev.pros = ., stringsAsFactors = F)
})
# 6. Cons
cons <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".description .row:nth-child(2) .mainText:not([class*='toggleBodyOff'])") %>%
html_text() %>% data.frame(rev.cons = ., stringsAsFactors = F)
})
# 7. Helpful
helpful <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".tight") %>%
html_text() %>% data.frame(rev.helpf = ., stringsAsFactors = F)
})
# Combine into a single data frame
df <- data.frame(date, summary, rating, title, pros, cons, helpful)
# Set URL (.com English canonical link)
baseurl <- "https://www.glassdoor.com/Reviews/Company-Reviews-"
companyNum <- "E40371"
sort <- ".htm?sort.sortType=RD&sort.ascending=true"
# How many total number of reviews? It will determine the maximum page results to iterate over.
totalReviews <- read_html(paste(baseurl, companyNum, sort, sep = "")) %>%
html_nodes(".tightVert.floatLt strong, .margRtSm.minor") %>%
html_text() %>%
sub("Found | reviews", "", .) %>%
sub(",", "", .) %>%
as.integer()
maxResults <- as.integer(ceiling(totalReviews/10))    #10 reviews per page, round up to whole number
# 1. Date
date <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".date.subtle.small, .featuredFlag") %>%
html_text() %>% data.frame(rev.date = ., stringsAsFactors = F)
})
# 2. Summary
summary <- map_df(1:maxResults, function(i) {
cat(" P", i, sep = "")
pg <- read_html(GET(paste(baseurl, companyNum, "_P", i, sort, sep = "")))
pg %>% html_nodes(".reviewLink .summary:not([class*='toggleBodyOff'])") %>%
html_text() %>% data.frame(rev.sum = ., stringsAsFactors = F)
})
summary
library(tidyverse)
source("R/scrape.R")
library(tidyverse)
source("R/scrape.R")
#> Loading libraries...
#> Sourcing functions...
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
# with random time
# filter for stuff we successfully extracted
reviews <- bind_rows(Filter(Negate(is.null), out), .id = "page")
reviews
out <- lapply(pages, function(page) {
Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
out
library(tidyverse)
source("R/scrape.R")
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
library(tidyverse)
source("R/scrape.R")
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
reviews
out
real_estate <- read_html(
"https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
)
real_estate <- read_html(
"https://www.glassdoor.com/Reviews/Google-Reviews-E9079.htm"
)
real_estate
flats <- real_estate %>%
html_nodes(".reviewLink") %>%
html_text()
flats
real_estate <- read_html(
"https://www.glassdoor.com/Reviews/Google-Reviews-E9079.htm?countryRedirect=false"
)
flats <- real_estate %>%
html_nodes(".reviewLink") %>%
html_text()
flats
real_estate <- read_html(
"https://www.glassdoor.com/Reviews/Google-Reviews-E9079.htm?countryRedirect=true"
)
flats <- real_estate %>%
html_nodes(".reviewLink") %>%
html_text()
flats
real_estate <- read_html(
"https://www.glassdoor.com/Reviews/Google-Reviews-E9079.htm?countryRedirect=true"
)
flats <- real_estate %>%
html_nodes(".reviewLink") %>%
html_text()
flats
real_estate <- read_html(
"https://www.glassdoor.com/Reviews/Google-Reviews-E9079.htm?countryRedirect=true"
)
flats <- real_estate %>%     html_nodes(xpath = "//*[contains(@id, 'empReview')]") %>%
html_attr("id")
flats <- real_estate %>% html_nodes(xpath = "//*[contains(@id, 'empReview')]") %>%
html_attr("id")
flats
source("R/scrape.R")
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
out
get_review_datetime <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[1]/div/time')
real_estate %>%
html_nodes(xpath = x) %>%
html_attr("datetime")
}
get_review_datetime
x
}
flats <- real_estate %>% html_nodes(xpath = '//*[@id="{review_id}"]/div/div[1]/div/time') %>%
html_nodes(xpath = x) %>%
html_attr("datetime")
flats
# Set URL
baseurl <- "https://www.glassdoor.com/Reviews/"
company <- "Tesla-Reviews-E43129"
sort <- ".htm?sort.sortType=RD&sort.ascending=true"
# How many total number of reviews? It will determine the maximum page results to iterate over.
totalreviews <- read_html(paste(baseurl, company, sort, sep="")) %>%
html_nodes(".margBot.minor") %>%
html_text() %>%
sub(" reviews", "", .) %>%
sub(",", "", .) %>%
as.integer()
maxresults <- as.integer(ceiling(totalreviews/10))    #10 reviews per page, round up to whole number
# Scraping function to create dataframe of: Date, Summary, Rating, Title, Pros, Cons, Helpful
df <- map_df(1:maxresults, function(i) {
Sys.sleep(sample(seq(1, 5, by=0.01), 1))    #be a polite bot. ~12 mins to run with this system sleeper
cat("boom! ")   #progress indicator
pg <- read_html(paste(baseurl, company, "_P", i, sort, sep=""))   #pagination (_P1 to _P163)
data.frame(rev.date = html_text(html_nodes(pg, ".date.subtle.small, .featuredFlag")),
rev.sum = html_text(html_nodes(pg, ".reviewLink .summary:not([class*='hidden'])")),
rev.rating = html_attr(html_nodes(pg, ".gdStars.gdRatings.sm .rating .value-title"), "title"),
rev.title = html_text(html_nodes(pg, "#ReviewsFeed .hideHH")),
rev.pros = html_text(html_nodes(pg, "#ReviewsFeed .pros:not([class*='hidden'])")),
rev.cons = html_text(html_nodes(pg, "#ReviewsFeed .cons:not([class*='hidden'])")),
rev.helpf = html_text(html_nodes(pg, ".tight")),
stringsAsFactors=F)
})
totalreviews
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
source("R/scrape.R")
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
library(rvest)
library(xml2)
library(dplyr, warn.conflicts = FALSE)
library(glue)
library(stringr)
read_page <- function(url, page) {
glue("{url}_P{page}.htm") %>%
read_html()
}
get_review_ids <- function(.data) {
.data %>%
html_nodes(xpath = "//*[contains(@id, 'empReview')]") %>%
html_attr("id")
}
url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
page <- read_page(url, 1)
review_ids <- get_review_ids(page)
get_review_title <- function(.data, review_id) {
# this is the old one
# x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/h2/a')
# this is the new one
x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/div[1]/h2/a')
.data %>%
html_nodes(xpath = x) %>%
html_text() %>%
str_remove_all(., '"')
}
get_review_title(page, review_ids[1])
?glue
suppressPackageStartupMessages({
library(rvest)
library(xml2)
library(dplyr)
library(lubridate)
library(glue)
library(stringr)
library(tidyr)
library(janitor)
})
message("Sourcing functions...")
read_page <- function(url, page) {
glue("{url}_P{page}.htm?countryRedirect=true") %>%
read_html()
}
get_review_ids <- function(.data) {
.data %>%
html_nodes(xpath = "//*[contains(@id, 'empReview')]") %>%
html_attr("id")
}
get_review_datetime <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[1]/div/time')
.data %>%
html_nodes(xpath = x) %>%
html_attr("datetime")
}
clean_review_datetime <- function(x) {
x <- trimws(sub("(GMT-).*", "", x))
parse_date_time(x, "a b d y H:M:S", tz = "gmt")
}
get_review_title <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/div[1]/h2/a')
.data %>%
html_nodes(xpath = x) %>%
html_text() %>%
str_remove_all(., '"')
}
get_review_title(page, review_ids[1])
page
review_ids
get_review_title
clean_review_datetime(page, review_ids[1]))
clean_review_datetime(page, review_ids[1])
clean_review_datetime(page)
get_employeer_pros <- function(.data, review_id) {
x <- glue('//*[@id="empReview_37753881"]/div/div[2]/div[2]/div[4]/p[2]')
.data %>%
html_nodes(xpath = x) %>%
html_text()
}
get_employeer_pros <- function(.data) {
x <- glue('//*[@id="empReview_37753881"]/div/div[2]/div[2]/div[4]/p[2]')
.data %>%
html_nodes(xpath = x) %>%
html_text()
}
get_review_title(page, review_ids[1])
get_employeer_pros(page)
get_employeer_pros <- function(.data) {
x <- glue('//*[@id="empReview_37753881"]/div/div[2]/div[2]/div[4]/p[2]/span')
.data %>%
html_nodes(xpath = x) %>%
html_text()
}
get_employeer_pros(page)
get_employeer_pros <- function(.data) {
x <- glue('# //*[@id="empReview_37753881"]/div/div[2]/div[2]/div[2]/div[1]/p[2]/span')
.data %>%
html_nodes(xpath = x) %>%
html_text()
}
page
review_ids[1]
get_employeer_pros(page)
get_employeer_pros <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/div[2]/div[1]/p[2]/span')
.data %>%
html_nodes(xpath = x) %>%
html_text()
}
get_employeer_pros(page, review_ids[1])
get_employeer_pros(page, review_ids[2])
get_employeer_pros(page, review_ids[3])
get_employeer_pros <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/div[2]/div[1]/p[2]')
.data %>%
html_nodes(xpath = x) %>%
html_text()
}
get_employeer_pros(page, review_ids[3])
get_employeer_pros(page, review_ids[3])
get_employeer_pros <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/div[2]/div[1]/p[2]')
.data %>%
html_nodes(xpath = x) %>%
html_text()
}
get_employeer_pros
get_employeer_pros(page, review_ids[3])
library(tidyverse)
source("R/scrape.R")
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
out
# remove any duplicates, parse the review time
reviews %>%
distinct() %>%
mutate(
review_time = clean_review_datetime(review_time_raw),
page = as.numeric(page)
) %>%
select(
page,
review_id,
review_time_raw,
review_time,
review_title,
employee_role,
employee_history,
employeer_pros,
employeer_cons,
employeer_rating,
work_life_balance,
culture_values,
career_opportunities,
compensation_and_benefits,
senior_management,
employee_location
) %>%
glimpse()
get_review_title(page, review_ids[1])
get_review_ids <- function(.data) {
.data %>%
html_nodes(xpath = "//*[contains(@id, 'empReview')]") %>%
html_attr("id")
}
get_review_title(page, review_ids[1])
get_review_ids(page, review_ids[1])
get_review_ids(page)
get_review_title(page, review_ids[1])
get_review_datetime(page, review_ids[1])
get_review_title <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/div[1]/h2/a')
.data %>%
html_nodes(xpath = x) %>%
html_text() %>%
str_remove_all(., '"')
}
get_review_title(page, review_ids[1])
get_employee_role(page, review_ids[1])
get_employee_history(page, review_ids[1])
get_employee_location(page, review_ids[1])
get_employeer_pros(page, review_ids[1])
get_employeer_cons(page, review_ids[1])
wrong_overall_rating <- function(.data, review_id) {
x <- glue('//*[@id="{review_id}"]/div/div[2]/div[2]/div[1]/span
/div[1]/div/div')
.data %>%
html_nodes(xpath = x) %>%
html_text() %>%
as.numeric()
}
get_overall_rating(page, review_ids[1])
library(tidyverse)
source("R/scrape.R")
#> Loading libraries...
#> Sourcing functions...
# apple_url <- "https://www.glassdoor.com/Reviews/Apple-Reviews-E1138"
google_url <- "https://www.glassdoor.com/Reviews/Google-Reviews-E9079"
# loop through n pages
pages <- 1:5
out <- lapply(pages, function(page) {
# Sys.sleep(1)
try_scrape_reviews(google_url, page)
})
# with random time
# filter for stuff we successfully extracted
reviews <- bind_rows(Filter(Negate(is.null), out), .id = "page")
reviews
out
get_sub_ratings(page, review_ids[1])
